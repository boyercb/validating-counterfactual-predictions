\section{Introduction} \label{sec:introduction}
Many common tasks in prediction modeling involve ``what if'' questions that call for predictions under conditions that are contrary to those prevalent when the training data are collected. When these changing conditions can be usefully thought of as resulting from  hypothetical interventions on variables relevant to the outcome, we refer to the task as counterfactual prediction \cite{hernan_second_2019,dickerman_counterfactual_2020,van_geloven_prediction_2020}. There exist several broad categories of modeling tasks that involve counterfactual predictions. 

First, a model may be deployed in a setting that differs from the setting that generated the training data in terms of patterns of treatment after baseline. These changes in treatment patterns could occur in a single target population over time due to treatment policy changes, commonly referred to as problems of ``domain adaption'' or ``dataset shift''  \cite{finlayson_clinician_2021, subbaswamy_development_2020}. Alternatively, it could occur when a model is transported from a source to a new target population that is similar (e.g., with respect to covariate distribution) except for differences in post-baseline treatment patterns. In both cases, differences between the training and deployment settings can cause the predictive performance of models to degrade. While, ideally, one would re-train the model using data in the new setting, due to cost or as a stop gap one might attempt instead to use the existing data to adapt or tailor the model to target the expected outcome under a hypothetical intervention to match treatment patterns in the target setting (i.e., the setting in which the model will be applied). Alternatively, one might attempt to use existing data to evaluate the counterfactual performance of the model, unadapted, in the target setting \cite{pajouheshnia_accounting_2017}. Both are examples of counterfactual prediction tasks.

Second, a model may be intended for use as a decision support tool, for instance, by clinicians to counsel patients about their risk of disease under alternative treatment plans. In certain rare circumstances, such as when the model is estimated in a trial in the target population which randomly assigned participants to the same alternative treatment strategies considered for decision support with no nonadherence or loss to follow up \cite{glasziou_evidence_1995,dahabreh_using_2016,kent_personalized_2018,kent_predictive_2020,hoogland_tutorial_2021}, the task may not require the additional formalism of counterfactual prediction. However, as is commonly the case, when the trial diverges from this ideal type, such as when patterns of adherence differ between the trial and the setting in which the model will be deployed, or when the model is trained using observational data \cite{schulam_reliable_2017-1,dickerman_counterfactual_2020}, the targeted predictions involve contrary-to-fact interventions, and thus are instances of counterfactual prediction.

Third, specific interventions in the training data may be viewed as pernicious, undesired, or incompatible with the target setting in which the model will be deployed and thus their removal may be attempted via prediction under a hypothetical intervention that eliminated them. For instance, an analyst may be interested in developing a model for the risk of death in the absence of surgery, but may only have available data collected in an observational setting where some of the patients ultimately received surgery, a problem sometimes described as ``treatment drop-in'' \cite{van_geloven_prediction_2020,sperrin_using_2018}. Provided that these hypothetical elimination interventions are sensible in the target setting and supported by the training data (in the sense that at least some individuals in the training data in fact follow them), attempting to estimate the treatment-free or treatment-na\"{i}ve risk is another exampls of a counterfactual prediction task.

Compared to factual prediction tasks, counterfactual prediction tasks require stronger assumptions that are closely related to those commonly invoked in the causal inference literature. They also require alternative methods that allow analysts to estimate and draw inferences about counterfactual estimands. Recently, several estimation methods have been proposed for counterfactual prediction that seek to re-tool existing causal inference approaches for use in the prediction setting \cite{lin_scoping_2021,sperrin_using_2018,dickerman_predicting_2022,schulam_reliable_2017-1}. However, these are not always accommodating of the fact that the predictors available at the time a model is to be used may not overlap with those available for training or those required to satisfy assumptions necessary for identifiability of the counterfactual estimand \cite{coston_counterfactual_2021}. Additionally, there has been comparatively little work to date (one exception is \textcite{coston_counterfactual_2020}) on how to estimate the performance of counterfactual prediction models independently of the method used to fit the model and whether it is misspecified. 

In this paper, we provide formal identifiability results for fitting a counterfactual prediction model. We derive and describe estimation methods for the case when a sufficient set of covariates is available during model training to control treatment-outcome confounding, but allow for the model itself to be conditional on a smaller subset of ``predictors'' that are available when deployed. We also provide identifiability results and novel estimators for multiple model performance measures including loss-based measures (such as the mean-squared error), the area under the receiver operating characteristics curve (AUC), and the calibration curve. We illustrate these methods using simulation and apply them to the task of developing a statin-na\"{i}ve risk prediction model for cardiovascular disease.

\section{Set up and notation} \label{sec:setup}
Let $Y$ be the outcome of interest, $X$ a vector of baseline covariates, and $A$ an indicator of treatment after baseline. We assume that data are independent realizations $\{(X_i, A_i, Y_i)\}_{i=1}^n$ from a source population in an observational setting in which treatment is not determined by the investigator but rather initiated according to the pattern $f^{obs}(A | X)$. The covariates in $X$ include a set sufficient to control confounding of the treatment-outcome relationship ($L$) as well as additional predictors of the outcome ($P$). An example causal directed acyclic graph (DAG) for this process is shown in Figure \ref{fig:dag1}. We allow for the possibility that only a subset of covariates $X^*$, chosen based on their availability and predictive potential rather than on whether they control confounding, will be use at the time predictions are to be made (note that $X^*$ can include components of both $L$ and $P$).

\begin{figure}[t]
    \centering
    \begin{tikzpicture}[> = stealth, shorten > = 1pt, auto, node distance = 2.5cm, inner sep = 0pt,minimum size = 0.5pt, semithick]
    \tikzstyle{every state}=[
      draw = white,
      fill = white
    ]
    \node[state] (l0) {$L$};
    \node[state] (a0) [right of=l0] {$A$};
    \node[state] (y1) [right of=a0] {$Y$};
    \node[state] (p0) [below of=l0] {$P$};
    \node[state] (u0) [above of=l0] {$U$};

    \path[->] (l0) edge node {} (a0);
    \path[->] (l0) edge [out=20, in=160, looseness=1.5] node {} (y1);

    \path[->] (a0) edge node {} (y1);
    
    \path[->] (p0) edge node {} (y1);

    \path[->] (u0) edge node {} (y1);
    \path[->] (u0) edge node {} (l0);
    \end{tikzpicture}
    \caption{Example causal directed acyclic graph (DAG) for prediction in a setting with a single time fixed treatment $A$ over follow up.}
    \label{fig:dag1}
\end{figure}

Let $Y^a$ denote the potential outcome under a hypothetical intervention which sets $A$ to $a$. Our objective is to build a model for the conditional expectation of the potential outcome in the target population under intervention $f^*(A|X)$ and assess its performance, where here we use $f(\cdot)$ generically to denote a density. For simplicity, in the main text we focus on simple intervention strategies where everyone receives treatment $A = a$, in which case the goal is to estimate $E[Y^a | X^*]$. Similarly, we assume, for now, that treatment is time-fixed, that is treatment, when initiated, is done so immediately after baseline and sustained over follow up or alternatively that its effect is independent of the time on treatment. In the Appendix, we broaden our results to more general intervention strategies, including random and dynamic regimes (Section \ref{sec:randomdynamic}), and to the case where treatment is time-varying (Section \ref{sec:timevarying}). Finally, we assume no censoring or loss to follow up. 

For the purposes of assessing model performance, we assume data are randomly split into a training set of $n_{train}$ observations and a test set of $n_{test}$ observations, with $n = n_{train} + n_{test}$. Let $D_{train}$ and $D_{test}$ be indicators of whether an observation is in the training set or test set respectively. We assume the model is to be trained in the training set and evaluated in the test set. Our results, however, apply equally to existing prediction models or (directly measured) biomarkers, in which case splitting is not necessary. To simplify exposition, we refer to a single test-train split throughout but, as we discuss further in Section \ref{sec:selection}, our set up easily accommodates more complex evaluation strategies such as cross-validation or methods based on bootstrapping. % Let $\mu(X^*)$ be a model, indexed by parameter $\beta$, and $\widehat{\mu}(X^*)$ be the ``fitted'' model using parameter estimates $\widehat{\beta}$. We allow for the possibility that model $\mu(X^*)$ is \textit{misspecified}. For a particular estimand $\psi(X^*)$, a model is correctly specified if there exists $\beta_0 \in \mathcal{B}$, where $\mathcal{B}$ is the parameter space of $\beta$, such that $\mu_{\beta_0}(X^*) = \psi(X^*)$ and the model is misspecified if no such $\beta_0$ exists. 

\section{Estimands for model fitting and performance} \label{sec:targets}
We distinguish between two possible counterfactual prediction tasks (i) fitting a model that targets a counterfactual prediction estimand and (ii) assessing the performance of an arbitrary model, whether tailored or not tailored to a specific setting, through comparison of model predictions against the potential outcome under the specified intervention. For the first task, the goal is to directly estimate the expected potential outcome under intervention $A = a$ conditional on covariates $X^*$ which are a subset of $X$, in other words the estimand is $\mu_a(X^*) \equiv \E[Y^a | X^*]$. As $\mu_a(X^*)$ depends on the unknown potential outcome $Y^a$, it is not a function of the observed data without additional assumptions. Therefore, we derive identifiability results for $\mu_a(X^*)$ and associated estimation procedures to inform model development. We refer to this task as ``model tailoring'' because the estimation procedure is tailored to the outcome mean in a target population under the specified intervention.

To evaluate the performance of a prediction model, we would like to estimate one or more performance measures.  A number of performance measures have been proposed in the prediction literature \cite{harrell_multivariable_1996, altman_what_2000, steyerberg_clinical_2019}. An example performance measure of interest is 
\begin{equation*}
    \psi(a) \equiv \E[(Y^a - \widehat{\mu}(X^*))^2 \mid D_{test} = 1]
\end{equation*}
where the squared error loss $(Y^a - \widehat{\mu}(X^*))^2$ quantifies the discrepancy between the potential outcome under treatment level $A = a$ and the model prediction $\widehat{\mu}(X^*)$ in terms of the squared difference. In the main text, we focus on the mean squared error as the performance measure of interest $\psi(a)$. In the Appendix, we extend our results to the case where the measure is any member of a generic class of loss functions for counterfactual outcomes $L(Y^a,  \widehat{\mu}(X^*))$ (Section \ref{sec:tf_proof}) as well as more complex risk-based measures such as the area under the receiver operating characteristics curve (AUC, Section \ref{sec:auc}), which depends on paired observations, and the calibration curve (Section \ref{sec:calib}), which is a functional. Importantly, all the measures considered are identifiable without assuming that the model $\mu(X^*)$ is correctly specified, in the sense that it need not converge to the true conditional expectation $\mu_a(X^*)$.

\section{Identifiability conditions} \label{sec:identifiability}
We assume the following identifiability assumptions which have been described in more detail elsewhere \cite{hernan_causal_2020, robins_new_1986, robins_graphical_1987}.

\begin{enumerate}
    \item[A1.] \textit{Consistency.} If $A = a$, then $Y^a = Y$ 
    \item[A2.] \textit{Conditional exchangeability.} $Y^a \perp\!\!\!\perp A \mid X$ 
    \item[A3.] \textit{Positivity.} For all $x$ with positive density, i.e. $f_X(x) > 0$, $\Pr[A = a \mid X = x] > 0$ 
\end{enumerate}

Consistency implies that observed outcomes among those with $A = a$ reflect potential outcomes under corresponding level of treatment. It would be violated if, for instance, there were interference, Hawthorne effects, or if there were multiple ``hidden'' versions of the treatment under consideration \cite{rubin_randomization_1980,dahabreh2019generalizing}. The latter may be particularly likely in prediction datasets where information on treatment is passively collected, for instance in a electronic healthcare record where they are not well defined or include hidden components. The exchangeability condition stipulates that treatment is conditionally independent of the potential outcome given covariates $X$. It is often referred to as the ``no unmeasured confounding'' assumption as it implies that there are no unmeasured variables that affect both treatment assignment and the outcome.  We require exchangeability conditional on $X$ but recall that the prediction model itself may be conditional on only a subset $X^*$. This allows, for instance, a high-dimensional covariate vector to be collected for model training, while the ultimate prediction tool may only be based on a handful of easy-to-obtain measurements at runtime. In contrast to a typical observational causal inference set up, we allow $X$ to include predictors that are not confounders (e.g., elements of $P$ instead of $L$ in DAG in Figure \ref{fig:dag1}). However, we require that conditioning on these predictors must not lead to a violation of Assumption A2, as would happen if the predictor were in fact a collider. Therefore subject-matter expertise should still guide the selection of the full set of covariates $X$. Finally, the positivity condition implies that there is a positive probability of observed treatment level $A = a$ in all strata of $X$ that have positive density in the population in the setting of interest. Positivity violations may also be common in prediction datasets where data are often assembled for multiple prediction tasks and without regard to who is eligible for treatment.


\section{Fitting a counterfactual prediction model} \label{sec:model}

As we show in Appendix Section \ref{sec:model_proof}, under the conditions above $\E[Y^a | X^*]$ is identified by
\begin{equation}\label{eqn:estimand1}
    \mu_a(X^*) = \E[\E[Y \mid X, A = a, D_{train} = 1] \mid X^*, D_{train} = 1]
\end{equation}
or, equivalently, using an inverse probability weighted expression 
\begin{equation}\label{eqn:estimand2}
    \mu_a(X^*) = \E\left[\frac{I(A = a)}{\Pr[A = a \mid X, D_{train} = 1]} Y \Big| X^*, D_{train} = 1\right]
\end{equation}
The two expressions for $\mu_a(X^*)$ suggest possible approaches for estimating the counterfactual prediction model from the training data. 

One approach, based on equation \ref{eqn:estimand1}, is to subset to participants with corresponding treatment level $A = a$ in the training data and fit a model $\mu(X)$ for the observed $Y$ conditional $X$, i.e. $\E[Y | X, A = a] = \mu(X)$. Then when the desired predictors $X^*$ are a subset of $X$, the covariates sufficient to ensure exchangeability,  predictions are marginalized (standardized) over the covariates in $X$ that are not in $X^*$. When the dimension of $X^*$ is small, this can be done nonparametrically; however, when $X^*$ is higher-dimensional, an additional modeling step will be required either (1) modeling the estimated $\widehat{\mu}(X)$ as a function of $X^*$, i.e. $\E[\widehat{\mu}(X) | X^*]$ or (2) modeling the conditional density of $X$ given $X^*$, i.e. $f(X | X^*)$.  The resulting predictions will be consistent for $\E[Y^a | X^*]$ provided all models are correctly specified. An alternative suggested in \cite{dickerman_predicting_2022}, would be to simulate samples from the model $\mu(X)$ and fit a second stage model $\mu^*(X^*)$ using only the subset of the predictors of interest. This is similar to the first standardization method above and has the advantage that the second stage model may be developed using an outcome with same support as the original $Y$.

A second approach, based on equation \ref{eqn:estimand2}, is to fit a weighted model $\mu(X^*)$, using for instance weighted maximum likelihood, with weights equal to the probability of receiving treatment level $A = a$ conditional on covariates $X$ necessary to ensure exchangeability, i.e., sample analogs of $W(a) = \frac{I(A = a)}{\Pr[A = a \mid X, D_{train} = 1]}$. This is the basis for previously proposed methods for counterfactual prediction based on inverse probability of treatment weighting \cite{sperrin_using_2018, van_geloven_prediction_2020}. Note that, as before, it is possible to specify a subset of predictors $X^*$ used in the prediction model $\mu(X^*)$ as compared to the full set of covariates $X$ required for exchangeability which are only necessary for defining the weights $W$, however a second marginalization or simulation step is not required. This means estimating a counterfactual prediction model using the weighting approach can be accomplished more easily using off-the-shelf software.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Assessing model performance} \label{sec:performance}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Using the same identifiability conditions, in Appendix Section \ref{sec:tf_proof}, we show the model performance measure $\psi(a)$ is identifiable using data from the test set through the expression
\begin{equation}\label{eqn:mse1}
    \psi(a) \equiv \E\left[\E[(Y - \widehat{\mu}(X^*))^2 \mid X, A=a, D_{test} = 1] \mid D_{test} = 1\right]
\end{equation}
or, equivalently, using an inverse probability weighted expression, 
\begin{equation}\label{eqn:mse2}
    \psi(a) = \E\left[\frac{I(A = a)}{\Pr[A = a \mid X, D_{test} = 1]}(Y - \widehat{\mu}(X^*))^2 \mid D_{test} = 1\right]
\end{equation}
regardless of the model $\widehat{\mu}(X^*)$ (e.g.,~regardless of whether it has been tailored to target $\E[Y^a \mid X]$ or if it is correctly specified). 

As before, the two expressions suggest two different approaches for the estimation of model performance using the test data alone. 

First, using the sample analog of expression (\ref{eqn:mse1}), an estimator of the target MSE is 
\begin{equation}\label{eqn:cl_estimator}
    \widehat{\psi}_{CL}(a) = \frac{1}{n_{test}} \sum_{i=1}^nI(D_{test, i} = 1)\widehat{h}_a(X_i)
\end{equation}
where $\widehat{h}_a(X)$ is an estimator for the conditional loss $\E[(Y - \widehat{\mu}(X^*))^2 \mid X, A=a, D_{test} = 1]$ and from the perspective of estimating $\psi(a)$ may be considered a nuisance function. To keep notation simple, we suppress the dependency of $\widehat{h}_a(X)$ on $\widehat{\mu}$. When the dimension of $X$ is small it may be possible to estimate $\widehat{h}_a(X)$ nonparametrically. In almost all practical cases though some form of modeling will be required; in these cases, $\widehat{\psi}_{CL}(a)$ is a consistent estimator for $\psi(a)$ as long as the model for $\widehat{h}_a(X)$ is correctly specified.

Next, using the sample analog of expression (\ref{eqn:mse2}), an alternative weight-based estimator of the target MSE is 
\begin{equation}\label{eqn:ipw_estimator}
    \widehat{\psi}_{IPW}(a) = \frac{1}{n_{test}} \sum_{i=1}^n \frac{I(A_i = a, D_{test, i} = 1)}{\widehat{e}_a(X_i)}(Y_i - \widehat{\mu}(X^*_i))^2
\end{equation}
where $\widehat{e}_a(X)$ is another nuisance function estimating the probability of receiving treatment level $A = a$ conditional on $X$, i.e. $\Pr[A = a \mid X, D_{test} = 1]$. Again, when the dimension of $X$ is small it may be possible to use the sample analog of $\widehat{e}_a(X)$, but in most cases, it will have to be modeled. In these cases, $\widehat{\psi}_{IPW}(a)$ is a consistent estimator of $\psi(a)$ as long as the model for $\widehat{e}_a(X)$ is correctly specified.

The conditional loss estimator (\ref{eqn:cl_estimator}) relies on correctly specifying the model for the conditional loss and the weighting estimator (\ref{eqn:ipw_estimator})  relies on correctly specifying the model for the probability of treatment. In some settings, one estimator may be preferred over the other: when more is known about the mechanism for ``assigning'' treatment or when the outcome is very rare, the weighting estimator may be preferred \cite{robins_estimating_1992,braitman_rare_2002}; when the process that gives rise to the outcomes is well understood the conditional loss estimator may be preferred. In practice, however, both models may be difficult to specify correctly. Using data-adaptive and more flexible machine learning estimators for estimation of these nuisance models offers the possibility of capturing more complex data generation processes. These data-adaptive estimators generally have slower rates of convergence than the $\sqrt{n}$ rates of parametric models and therefore will not yield asymptotically valid confidence intervals \cite{chernozhukov_doubledebiased_2018}. To address this challenge, we can use a doubly-robust estimator which combines models for the conditional loss $\widehat{h}_a(X)$ and the probability of treatment $\widehat{e}_a(X)$, such as
\begin{equation}
    \widehat{\psi}_{DR}(a) = \frac{1}{n_{test}} \sum_{i=1}^n I(D_{test,i} = 1) \left[ \widehat{h}_a(X_i) + \frac{I(A_i = a)}{\widehat{e}_a(X_i)} \left\{ (Y_i - \widehat{\mu}(X^*_i))^2 - \widehat{h}_a(X_i)\right\}\right]
\end{equation}
As we show in Appendix section \ref{sec:dr}, under mild regularity conditions \cite{robins_higher_2008}, this estimator will be consistent if one of $\widehat{h}_a(X)$ and $\widehat{e}_a(X)$ is correctly specified. It also permits the use of machine learning or data-adaptive estimators that converge at rate slower than $\sqrt{n}$, thus allowing for more flexible estimation of the nuisance functions. This is due to the fact that the empirical process terms governing the convergence of $\widehat{\psi}_{DR}$ involve a product of the estimation errors for $\widehat{h}_a(X)$ and $\widehat{e}_a(X)$ which converge under the weaker condition that only the \textit{combined} rate of convergence for both nuisance functions is at least $\sqrt{n}$ \cite{chernozhukov_doubledebiased_2018}. In Appendix section \ref{sec:dr}, we present results for general loss functions under static interventions. We leave the derivation of doubly robust estimators for the AUC and calibration curve as well as random and dynamic regimes for future work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Model and tuning parameter selection} \label{sec:selection}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Up to this point, we have assumed that $\mu(X^*)$ is a pre-specified parametric model and ignored any form of model selection (e.g., variable or other specification search) or data-adaptive tuning parameter selection, which may be the case when using an existing (validated) model. In many cases, however, analysts have to select between multiple models or perform a data-adaptive search through a parameter space for tuning parameter selection when developing a prediction model \cite{steyerberg_clinical_2019}. To avoid overfitting, analysts typically use methods such as cross-validation or the bootstrap to perform selection. These techniques rely on optimizing some measure of model performance, such as the MSE.

When performing model or tuning parameter selection for counterfactual prediction, the results from the previous sections suggest that the model performance measure should be targeted to the counterfactual performance if the intervention of interest were universally applied. For example, when using cross-validation for model selection the analyst splits the data into $K$ mutually exclusive ``folds'' and fits the candidate models using $K - 1$ of the folds and estimates the performance of each in the held out fold. This process is repeated $K$ times where each fold is left out once. The final performance estimate is the average of the $K$ estimates and the model with best overall performance is selected (or, alternatively, the tuning parameter with the best performance). When performing counterfactual prediction, at each stage in the procedure the analyst should use modified performance measures such as those in section \ref{sec:performance} above. Failure to do so, can lead to sub-optimal selection with respect to the counterfactual prediction of interest. 

%\section{Time-varying treatments} \label{sec:time-varying}

\section{Simulation experiments} \label{sec:simulation}
We performed two Monte Carlo simulation experiments to illustrate (i) the benefits of tailoring models to the correct counterfactual estimand of interest, (ii) the potential for bias when using na\"{i}ve estimators of model performance such as the MSE, (iii) the importance of correct specification of the nuisance models when estimating counterfactual performance, and (iv) the properties of the doubly-robust estimator under misspecification of the nuisance models. We adapt data generation processes previously used for transporting models between settings under covariate shift \cite{steingrimsson_transporting_2023, morrison_robust_2022}.

\subsection{Experiment 1}
We simulated treatment initiation at baseline based on the logistic model $\Pr[A=1 \mid X]=\operatorname{expit} (1.5-0.3 X)$, where predictors $X$ are drawn from $X \sim$ Uniform $(0,10)$. Under this model, about 50\% initiate treatment but those with higher values of $X$ are less likely to start treatment than those with lower values of $X$. We then simulated the outcome using the linear model $Y=1+X+0.5 X^2- 3A + \varepsilon$, where $\varepsilon \sim \mathcal{N}(0, X)$.  We set the total sample size to 1000 and the data were randomly split in a 1:1 ratio into a training and a test set. In this simulation, $X = X^*$. The full process may be written:
\begin{align*}
    X & \sim \text{Unif}(0, 10) \\
    A & \sim \text{Bernoulli}\{\operatorname{expit}(-1.5 + 0.3 X)\} \\
    Y & \sim \text{Normal}(1 + X + 0.5 X^2 - 3 A, X)
\end{align*}    
 
Our goal was to estimate a model and evaluate it's performance in the same population where, contrary to fact, treatment was universally withheld, i.e. we targeted $\E[Y^{a=0} \mid X]$. Note that, for simplicity, in this case predictor variables are the same set necessary to control confounding $X = X^*$. Under this data generating mechanism, the MSE under no treatment is larger than the MSE under the natural course and identifiability conditions 1-3 are satisfied. We considered two parametric model specifications $\mu(X^*; \beta)$ and $\widetilde{\mu}(X^*, \beta)$:
\begin{enumerate}
    \item a correctly specified linear regression model that included the main effects of $X$ and $X^2$, i.e. $\mu(X; \beta) = \beta_0 + \beta_1 X + \beta_2 X^2$.
    \item a misspecified linear regression model that only included the linear effect of $X$, i.e. $\widetilde{\mu}(X; \beta) = \beta_0 + \beta_1 X$.
\end{enumerate} 
For each specification, we also considered two estimation strategies: one using ordinary least squares regression (OLS) and ignoring treatment initiation and the other using weighted least squares regression (WLS) where the weights were equal to the inverse of the probability of being untreated. As discussed above the latter specifically targets the counterfactual estimand under no treatment. Finally, we considered two approaches for estimating the performance of the models in the test set: a na\"{i}ve estimate of the MSE using observed outcome values, i.e., 
$$\widehat{\psi}_{Na\ddot{i}ve}(a) = \frac{1}{n_{test}} \sum_{i=1}^n I(D_{test,i} = 1) (Y_i - \widehat{\mu}(X_i))^2,$$ 
and the inverse-probability weighted estimator $\widehat{\psi}_{IPW}(a)$ from section \ref{sec:performance}. For the latter, we fit a correctly specified logistic regression model for $e_a(X)$, i.e. $e_a(X) = \operatorname{expit}(\alpha_0 + \alpha_1 X)$, in the test set to estimate the weights. Lastly, we also calculated the ``true'' MSE under intervention to withold treatment by generating test data under same process as above but setting the treatment $A$ to $a = 0$ for everyone and then averaging across simulations.

\begin{table}[t]
    \centering
    \caption{Simulation results for experiment 1 comparing the performance of OLS and WLS models using the na\"{i}ve and inverse probability weighting (IPW) estimators of the MSE.}
    \begin{threeparttable}
        \begin{tabular}{p{3cm}R{1.25cm}R{1.25cm}R{1.25cm}}
        \toprule
        Model $\mu(X)$ & Na\"{i}ve & IPW & Truth  \\
        \midrule
        Correct & & & \\
        \hspace{1em}OLS & 2.9 & 3.6 & 3.6\\
        \hspace{1em}WLS & 5.5 & 1.0 & 1.0\\
        \addlinespace[0.25em]
        Misspecified & & & \\
        \hspace{1em}OLS & 16.8 & 17.5 & 17.5\\
        \hspace{1em}WLS & 19.5 & 15.0 & 15.0\\
        \bottomrule
        \end{tabular}
        \begin{tablenotes}
        \item \noindent Correct and misspecified refers to the specification of the  prediction model $\mu(X)$. OLS = model estimation using ordinary least squares regression (unweighted); WLS = model estimation using weighted least squares regression with weights equal to the inverse probability of being untreated. Results were averaged over 10,000 simulations. The true counterfactual MSE was obtained using numerical methods. 
        \end{tablenotes}
        \end{threeparttable}
    
\end{table}

Table 1 shows the results of the experiment based on 10,000 simulations. In general, correctly specified models yielded smaller average MSE than misspecified models. Comparing the performance of OLS and WLS estimation, when using $\widehat{\psi}_{Na\ddot{i}ve}(a)$, the na\"{i}ve estimator of the MSE, OLS seemed to produce better predictions than WLS when correctly specified (average MSE of 2.9 vs. 5.5) as well as when misspecified (average MSE of 16.8 vs. 19.5). In contrast, when using $\widehat{\psi}_{IPW}(a)$, the inverse-probability weighted estimate of the MSE, WLS performed better than OLS both when the model was correctly specified (average MSE of 1.0 vs. 3.6) and when misspecified (average MSE of 15.0 vs. 17.5). For reference, in the last column we show the true counterfactual MSE that would be obtained if one had access to the potential outcomes (obtained via numerical methods). We found that the average of the inverse probability weighted estimator across the simulations was equivalent to this quantity for all specifications and for both OLS and WLS estimation. This suggests that only the modified estimators of model performance in section \ref{sec:performance} are able to accurately estimate the counterfactual performance of the model. Indeed, under this data generation process, if one were to use the na\"{i}ve estimator one might erroneously conclude that the OLS model is the better choice.


\subsection{Experiment 2}

In the previous experiment, we assumed the nuisance models for the MSE were correctly specified. We now consider estimation of model performance in the more likely case that nuisance models are misspecified. Using the results from Section \ref{sec:auc} in the Appendix, we also estimate the area under the receiver operating characteristics curve (AUC). We simulated treatment initiation over follow up $A$ based on the logistic model $\operatorname{Pr}[A=1 \mid X]=\operatorname{expit}(0.5 - 2 X_1 + 3 X_1^2 + 2 X_2 - X_3)$, where $X$ is now a vector of predictors drawn from a 3-dimensional multivariate normal with mean vector $\mu = (0.2, 0, 0.5)$ and covariance matrix $\Sigma = \text{diag}(0.2, 0.2, 0.2)$. This resulted in expected treatment initiation over follow up of 55\%. We also simulated a binary outcome from a Bernoulli distribution with mean $\operatorname{expit}(0.2 + 3 X_1 - 2 X_1^2 + 2 X_2 + X_3 - 2 A)$, implying an average probability of the outcome of 66\% among the untreated and 32\% among treated. Again, we set the total sample size to 1000 and randomly split the data in a 1:1 ratio into a training and a test set. 
\begin{align*}
    X & \sim \text{MVN}(\mu, \Sigma) \\
    A & \sim \text{Bernoulli}\left\{\text{expit}\left(0.5 - 2 X_1 + 3 X_1^2 + 2 X_2 - X_3\right)\right\} \\
    Y & \sim \text{Bernoulli}\left\{\text{expit}\left(0.2 + 3 X_1 - 2 X_1^2 + 2 X_2 + X_3 - 2 A\right)\right\}
\end{align*}

Our prediction model was a main effects logistic regression model fit in the training data, i.e. $\mu\left(X^*\right) = \operatorname{expit}(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3)$. This model was misspecified with respect to the true data generating process. We assessed the counterfactual performance of the model in an untreated population using the AUC and the MSE, which for a binary outcome is equivalent to the Brier score \cite{brier_verification_1950}. In general, positing a parametric model for $h_0(X)=\mathrm{E}[(Y-\mu\left(X^*\right))^2 \mid X, A=0]$ may be difficult as the outcome is a squared difference. For binary outcomes, however, expanding the square shows that to estimate $h_0(X)$ it is enough to estimate $\operatorname{Pr}[Y=1 \mid X, A=0]$, which is the approach we used. To determine the effect of the specification of nuisance models $e_a(X)$ and $h_a(X)$ on performance estimates, we compared four estimators of AUC and MSE (${\psi}_{Na\ddot{i}ve}(0)$, ${\psi}_{IPW}(0)$, ${\psi}_{CL}(0)$, and ${\psi}_{DR}(0)$) using different combinations of correctly specified and misspecified models for $e_a(X)$ and $h_a(X)$:
\begin{enumerate}
    \item Correct $e_a(X)$ - main effects logistic regression model with linear and quadratic terms.
    \item Misspecified $e_a(X)$ - main effects logistic regression model with linear terms only terms.
    \item Correct $h_a(X)$ - main effects logistic regression model with linear and quadratic terms.
    \item Misspecified $h_a(X)$ - main effects logistic regression model with linear terms only terms.
\end{enumerate}
Finally, we also considered using more flexible estimation techniques for nuisance terms $e_a(X)$ and $h_a(X)$. Specifically, we fit generalized additive models for both using the \texttt{mgcv} package in $\mathrm{R}$ entering all covariates as splines using the default options in the \texttt{gam} function.

\begin{table}[t]
    \centering
    \footnotesize
    \caption{Simulation results for experiment 2 comparing the performance of the na\"{i}ve, conditional loss (CL), inverse-probability weighting (IPW), and doubly robust (DR) estimators of the MSE and AUC under correct and misspecified nuisance models.}
 
\begin{threeparttable}
    \begin{tabular}{lcccccccc}
    \toprule
    \multicolumn{1}{c}{ } & \multicolumn{4}{c}{MSE} & \multicolumn{4}{c}{AUC} \\
    \cmidrule(l{3pt}r{3pt}){2-5} \cmidrule(l{3pt}r{3pt}){6-9}
    Estimator & Mean & $\sqrt{n}\times\text{SD}$ & $\sqrt{n}\times\text{Bias}$ & Percent & Mean & $\sqrt{n}\times\text{SD}$ & $\sqrt{n}\times\text{Bias}$ & Percent\\
    \midrule
    Na\"{i}ve & 0.207 & 0.176 & -0.140 & -2.1 & 0.742 & 0.491 & -1.335 & -5.4\\
    \addlinespace[0.3em]
    \multicolumn{9}{l}{Correct}\\
    \hspace{1em}CL & 0.212 & 0.333 & 0.015 & 0.2 & 0.783 & 0.767 & -0.045 & \vphantom{1} -0.2\\
    \hspace{1em}IPW & 0.212 & 0.517 & 0.011 & 0.2 & 0.782 & 1.258 & -0.062 & \vphantom{1} -0.3\\
    \hspace{1em}DR & 0.211 & 0.454 & 0.000 & 0.0 & 0.783 & 1.192 & -0.028 & -0.1\\
    \addlinespace[0.3em]
    \multicolumn{9}{l}{$e_a(X)$ misspecified}\\
    \hspace{1em}CL & 0.212 & 0.333 & 0.015 & 0.2 & 0.783 & 0.767 & -0.045 & -0.2\\
    \hspace{1em}IPW & 0.221 & 0.358 & 0.316 & 4.7 & 0.762 & 0.876 & -0.699 & -2.8\\
    \hspace{1em}DR & 0.212 & 0.349 & 0.016 & 0.2 & 0.782 & 0.841 & -0.066 & -0.3\\
    \addlinespace[0.3em]
    \multicolumn{9}{l}{$h_a(X)$ misspecified}\\
    \hspace{1em}CL & 0.217 & 0.356 & 0.194 & 2.9 & 0.777 & 0.803 & -0.224 & -0.9\\
    \hspace{1em}IPW & 0.212 & 0.517 & 0.011 & 0.2 & 0.782 & 1.258 & -0.062 & -0.3\\
    \hspace{1em}DR & 0.211 & 0.625 & 0.001 & 0.0 & 0.783 & 1.317 & -0.024 & -0.1\\
    \addlinespace[0.3em]
    \multicolumn{9}{l}{Both misspecified}\\
    \hspace{1em}CL gam & 0.213 & 0.348 & 0.052 & 0.8 & 0.782 & 0.800 & -0.063 & -0.3\\
    \hspace{1em}IPW gam & 0.214 & 0.422 & 0.075 & 1.1 & 0.778 & 1.032 & -0.181 & -0.7\\
    \hspace{1em}DR gam & 0.211 & 0.403 & 0.010 & 0.1 & 0.784 & 0.966 & -0.021 & -0.1\\
    Truth & 0.211 & &  &  & 0.784 & &  & \\
    \bottomrule
    \end{tabular}
    \begin{tablenotes}
    \item Average of estimates, estimated bias, estimated standard deviation (SD), and estimated relative bias for the na\"{i}ve empirical, weighting (IPW), conditional loss (CL), and doubly robust (DR) estimators. $\sqrt{n}$ is the number of observations in the test set. Here, $h_a(X)$ is a model for $\operatorname{Pr}[Y=1 \mid X, A=a]$ and $e_a(X)$ denotes a model for $\Pr[A = a|X]$. Relative bias is calculated as $(\text{estimator} -\text{truth})/\text{truth}$. Correct and Misspecified refer to the nuisance models, $e_a(X)$ or $h_a(X)$ or both. In the final rows, gam indicates that a generalized additive model was used to estimate nuisance models. Results were averaged over 10,000 simulations.
    \end{tablenotes}
    \end{threeparttable}
\end{table}

Table 2 shows the results from experiment 2. As in the previous experiment, the na\"{i}ve empirical estimators of the AUC and MSE were biased relative to the true counterfactual values with a relative bias of 2.1\% and $-5.4\%$ respectively. When all models were correctly specified, the weighting, conditional loss, and doubly robust estimators were all unbiased (absolute relative bias between 0.2\% to 0.3\%). When $\widehat e_a(X)$ was misspecified, the weighting estimator was biased (relative bias of 4.7\% and -2.8\%) but the conditional loss and doubly robust estimator were unbiased (absolute relative bias of 0.2\% to 0.3\%). Under misspecification of $h_a(X)$, the conditional loss estimator was biased (relative bias of 2.9\% and -0.9\%), but the weighting estimator and the doubly robust estimator were unbiased (absolute relative bias of 0.0\% to 0.3\%). When both models $e_a(X)$ and $h_a(X)$ were misspecified all estimators, including the doubly robust estimator, were biased. Finally, when a more flexible generalized additive model was used to estimate both $e_a(X)$ and $h_a(X)$, the doubly robust estimator was unbiased (absolute relative bias of 0.1\%). Across all scenarios, the weighting estimator generally had the largest standard errors and the conditional loss estimator had the smallest standard errors.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Application to prediction of statin-na\"{i}ve risk} \label{sec:results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

We apply the proposed methods to evaluate the performance of two counterfactual prediction models targeting the statin-na\"{i}ve risk of cardiovascular disease: that is the risk in the same population if, contrary to fact, statins had been withheld. We compare one model that was explicitly tailored for the counterfactual estimand of interest and a second that was not. 

\subsection{Study design and data}
The Multi-Ethnic Study on Atherosclerosis (MESA) study is a population-based sample of 6,814 men and women aged 45 to 84 drawn from six communities (Baltimore; Chicago; Forsyth County, North Carolina; Los Angeles; New York; and St. Paul, Minnesota) in the United States between 2000 and 2002. The design, sampling procedures, and collection methods of the study have been described previously \cite{bild_multi-ethnic_2002}. Study teams conducted five examination visits between 2000 and 2011 in 18 to 24 month intervals focused on the prevalence, correlates, and progression of subclinical cardiovascular disease. These examinations included assessments of lipid-lowering medication use (primarily statins), as well as assessments of cardiovascular risk factors such as systolic blood pressure, serum cholesterol, cigarette smoking, height, weight, and diabetes. 

In a previous analysis, we used data from the MESA study to emulate a trial comparing continuous statin use versus no statins and benchmarked our results against those from published randomized trials. To construct a model of the statin-na\"{i}ve risk, we then emulated a single arm trial in which no one started statins over a 10-year follow up period. To determine trial eligibility, we followed the AHA guidelines \cite{grundy_scott_m_2018_2019} on statin use which stipulate that patients aged 40 to 75 with serum LDL cholesterol levels between 70 mg/dL and 190 mg/dL and no history of cardiovascular disease should initiate statins if their (statin-na\"{i}ve) risk exceeds 7.5\%. Therefore, we considered MESA participants who completed the baseline examination, had no previous history of statin use, no history of cardiovascular disease, and who met the criteria described in the guidelines (excluding the risk threshold) as eligible to participate in the trial. The primary endpoint was time to atherosclerotic cardiovascular disease (ASCVD), defined as nonfatal myocardial infarction, coronary heart disease death, or ischemic stroke. 

Follow up began at the second examination cycle to enable a ``wash out'' period for statin use and to ensure adequate pre-treatment covariates to control confounding (some were taken from the first cycle and others from the second). In the original analysis, we constructed a sequence of nested trials starting at each exam, however here for simplicity we limited our attention to the first trial. We used the questionnaire in examinations three through five to determine statin initiation over the follow up period. Because the exact timing of statin initiation was not known with precision, we estimated it by drawing a random month between the current and previous examinations (sensitivity analyses conducted in original study).

Of the 6,814 MESA participants who completed the baseline examination, 4,149 met the eligibility criteria for our trial emulation. There were 288 ASCVD events and 190 non-ASCVD deaths. For simplicity, here we dropped those lost to follow up and who first have competing events although in practice both can be accommodated in our framework for evaluating the performance of a counterfactual prediction model. For model training and evaluation, we further randomly split the dataset into training and test sets of equal size. 

\subsection{Model estimation and performance}

We compared two prediction models: one that was explicitly tailored to the statin-na\"{i}ve risk and a second that was not. Both models used the same regression specification with main effects of baseline predictors commonly used in cardiovascular risk prediction: age, sex, smoking status, diabetes history, systolic blood pressure, anti-hypertensive medication use and total and HDL serum cholesterol levels.

We tailored the first model for the statin-na\"{i}ve risk using inverse probability of censoring weights. In the emulated single arm trial, statin initiation can be viewed as ``non-adherence'' which can be adjusted for by inverse probability weighting, therefore we censored participants when they initiated statins. To calculate the stablized weights, we estimated two logistic regression models: one for the probability of remaining untreated given past covariate history (denominator model) and one for probability of remaining untreated given the selected baseline predictors (numerator model). The list of covariates in the weight models are given in section \ref{sec:covs} of the Appendix. To create a prediction model for the statin-na\"{i}ve risk, we used the estimated weights to fit a weighted logistic regression model conditional on the baseline predictors of interest. 

For comparison, we fit a second traditional (factual) prediction model by regressing the observed ASCVD event indicator on the same set of baseline predictors, but ignoring treatment initiation over the follow up period. This approach targets the ``natural course'' risk (i.e., the risk under the statin initiation policies that prevailed at the time of the study) rather than the statin-na\"{i}ve risk. We fit the model using standard logistic regression based on maximum likelihood.

To assess the performance of the models, we estimated the na\"{i}ve and counterfactual MSE in the test set. For the latter we used the conditional loss, inverse probability weighting, and doubly robust estimators of the MSE. Models for the initiation of treatment $e_a(X)$ and for the conditional loss $h_a(X)$ were implemented as main effects logistic regression models. As in the simulation example, to estimate the conditional loss it is sufficient to model the probability of the outcome alone. To quantify uncertainty, we used the non-parametric bootstrap with 1000 bootstrap replicates.


\begin{table}[t]
    \centering
    \caption{Estimated MSE in a statin-na\"{i}ve population for two prediction models using emulated trial data from MESA.}
    \begin{threeparttable}
        \begin{tabular}{lcccc}
        \toprule
        \multicolumn{1}{c}{ } & \multicolumn{2}{c}{MSE} & \multicolumn{2}{c}{AUC} \\
        \cmidrule(l{3pt}r{3pt}){2-3} \cmidrule(l{3pt}r{3pt}){4-5}
        Estimator & Logit & Weighted Logit & Logit & Weighted Logit\\
        \midrule
        Na\"{i}ve & 0.069 & 0.072 & 0.710 & 0.708\\
         & (0.003) & (0.003) & (0.013) & (0.014)\\
        CL & 0.086 & 0.085 & 0.719 & 0.727\\
         & (0.005) & (0.004) & (0.015) & (0.015)\\
        IPW & 0.109 & 0.099 & 0.753 & 0.778\\
         & (0.013) & (0.009) & (0.025) & (0.029)\\
        DR & 0.090 & 0.087 & 0.740 & 0.751\\
         & (0.006) & (0.005) & (0.023) & (0.023)\\
        \bottomrule
        \end{tabular}
        \centering
        \begin{tablenotes}[flushleft]
        \item The columns refer to the posited prediction model: Logit is an (unweighted) logistic regression model and weighted logit is a logistic regression model with inverse probability weights for remaining statin-free. The rows are the model performance estimates of the MSE and AUC. Na\"{i}ve is the empirical estimator using factual outcomes ($\widehat{\psi}_{Na\ddot{i}ve}(0)$), CL is the conditional loss estimator ($\widehat{\psi}_{CL}(0)$), IPW is the inverse probability weighting estimator ($\widehat{\psi}_{IPW}(0)$), DR is the doubly-robust estimator ($\widehat{\psi}_{DR}(0)$). Standard error estimates are shown in parentheses obtained via 1000 bootstrap replicates.
        \end{tablenotes}
        \end{threeparttable}
\end{table}

\subsection{Results}

Table 3 shows estimates of the AUC and MSE and the associated standard errors in a hypothetical statin-na\"{i}ve population for both prediction models using the na\"{i}ve empirical, conditional loss, weighting, and doubly robust estimators. Across both measures, the conditional loss, weighting, and doubly robust estimators yielded estimates that were worse (30-50\% greater MSE, 3-10\% lower AUC) than those of the na\"{i}ve empirical estimator, suggesting performance of both models in statin-na\"{i}ve population is worse than in the source population. Of the three estimators of the statin-na\"{i}ve performance, the weighting estimator had greater standard errors than the doubly robust estimator (by 10-100\%) as well as the conditional loss estimator (by 60-160\%). Consistent with the first simulation experiment, the inverse probability weighted logistic model, which was tailored to target the statin-na\"{i}ve risk, performed worse in the source population, but had lower MSE and higher AUC values in the counterfactual statin-na\"{i}ve population. There were sizeable differences (2-3 standard errors) in estimates of the MSE and AUC in the counterfactual statin-na\"{i}ve population across the proposed CL, IPW, and DR estimators. All three should give equivalent results in expectation if models for the nuisance functions $h_a(X^*)$ and $e_a(X^*)$ are correctly specified. Their divergence suggests at least one (but possibly both) of the nuisance functions may be misspecified. Finally, drawing on the results in section \ref{sec:calib} in the appendix, we estimate the counterfactual calibration of both prediction models in a statin-na\"{i}ve population using a weighted loess estimator where the weights are based on inverse probability of remaining statin free. Figure \ref{fig:calib} shows the results. As expected, the weighted prediction model, which was tailored to target the statin-na\"{i}ve risk, was better calibrated than the unweighted model, which generally underestimated the counterfactual risk.

\begin{figure}[p]
    \centering
    \includegraphics{../3_figures/calib.pdf}
    \caption{Risk calibration curves for counterfactual prediction models fit using logistic regression with and with out inverse probability weights for statin initiation. The rug plot shows distribution of risk predictions among those who develop ASCVD (top) and those who don't (bottom). The black curve is a local-regression smoothed and inverse probability weighted estimate of the calibration curve targeting the statin-na\"{i}ve risk.\label{fig:calib}}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion} \label{sec:discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Many practical prediction problems call for the development and evaluation of counterfactual prediction models, for example, when the treatment distribution changes between the time training data are obtained and when the model will be deployed, or when predictions are meant to inform decisions about treatment initiation. In this paper, we considered the setting when the counterfactual prediction model has to be developed using observational training data. We described how to estimate counterfactual prediction models and the conditions necessary to identify these models. Separately, we also discussed how to adjust common measures of model performance to estimate the performance of counterfactual prediction model. Importantly, results for performance measures were valid even when the prediction model is misspecified. A key insight was that performance measures that have not been tailored for counterfactual prediction will be biased. We also showed that performance under a hypothetical intervention can be assessed even if the prediction model itself is misspecified. We proposed estimators for these performance measures based on modeling the conditional loss, the probability of treatment, and a doubly robust estimator that can be used with data-adaptive estimators of either nuisance function. 

We focused on measures of performance under a particular treatment policy. However, prediction models may instead target the estimation of conditional treatment \textit{effects}, that is, comparisons between treatment regimes such as $\tau(X^*) = \E[Y^1 - Y^0 \mid X^*]$. In some cases, effects may be easier to communicate to end users or may be desirable to evaluate benefits versus harms of treatment initiation \cite{kent_predictive_2020}. However, absolute means and risks are common outputs of existing prediction models. Several authors have proposed model performance measures for conditional average treatment effects which are identifiable under similar assumptions to our own \cite{schuler_comparison_2018, rolling2014model, xu_calibration_2022, van2003unified, alaa_validating_2019}. From an estimation standpoint, methods for targeting the conditional average treatment effect and their performance have to balance the estimation of the conditional risk function for the outcome under different treatment levels with the estimation of the treatment effect function, with optimality depending on the relative smoothness of these functions \cite{kennedy_towards_2022}.

Throughout, we did not assume that the covariates needed to satisfy the exchangeability assumption were the same covariates used in the prediction model. This is an important aspect of our work because in practice predictors are often chosen based on their availability in a clinical setting rather than what would be optimal from a causal (or even predictive) perspective \cite{steyerberg_clinical_2019, coston_counterfactual_2021}. Moreover, our methods reflect the fact that confounding is a problem that needs to be adjusted for in the setting where the model is developed, whereas predictions often need to be optimized for the setting where the model will be applied (although as discussed below allowing the covariate distribution to differ between settings can introduce further subtleties).

One limitation of our approach is that it require that a set of covariates sufficient to ensure exchanageability can be identified at the time of training the model. Violations of this exchangeability condition can be examined in sensitivity analyses \cite{robins_sensitivity_2000,steingrimsson_sensitivity_2023}, for example, to explore how violations of this assumption might affect estimates of model performance. Further work may also examine identification of counterfactual prediction models and their performance under alternative identifiability conditions.

In this work, we assumed that the distribution of predictors is the same in the training and deployment setting. In many cases, however, the covariate distributions are also likely to differ between settings \cite{bickel_discriminative_2009, sugiyama_covariate_2007}. Like differences in treatment initiation, differences in covariate distributions may cause the performance of the prediction model to degrade, particularly when the model is misspecified. Methods for transporting prediction models from source to target populations which relate to our own have previously been proposed \cite{sugiyama_covariate_2007,bickel_discriminative_2007,sugiyama2012machine, steingrimsson_transporting_2023, li_estimating_2022, morrison_robust_2022} as have methods for transporting conditional average effects from trials to target populations with different covariate distributions \cite{mehrotra_transporting_2021, seamans_generalizability_2021, robertson_estimating_2021, robertson_regression-based_2023}. In future work, our results could be extended to allow for both differences in the distribution of treatment $f(A|X)$ and the covariates $f(X)$ between training and deployment settings.

