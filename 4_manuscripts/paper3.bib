@inproceedings{alaa_validating_2019,
  title = {Validating {{Causal Inference Models}} via {{Influence Functions}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Alaa, Ahmed and Schaar, Mihaela Van Der},
  year = {2019},
  month = may,
  pages = {191--201},
  publisher = {{PMLR}},
  issn = {2640-3498},
  abstract = {The problem of estimating causal effects of treatments from observational data falls beyond the realm of supervised learning \{\textemdash\} because counterfactual data is inaccessible, we can never observe the true causal effects. In the absence of "supervision", how can we evaluate the performance of causal inference methods? In this paper, we use influence functions \{\textemdash\} the functional derivatives of a loss function \{\textemdash\} to develop a model validation procedure that estimates the estimation error of causal inference methods. Our procedure utilizes a Taylor-like expansion to approximate the loss function of a method on a given dataset in terms of the influence functions of its loss on a "synthesized", proximal dataset with known causal effects. Under minimal regularity assumptions, we show that our procedure is consistent and efficient. Experiments on 77 benchmark datasets show that using our procedure, we can accurately predict the comparative performances of state-of-the-art causal inference methods applied to a given observational study.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/2UNYSEZK/Alaa and Schaar - 2019 - Validating Causal Inference Models via Influence F.pdf}
}

@article{altman_what_2000,
  title = {What Do We Mean by Validating a Prognostic Model?},
  author = {Altman, Douglas G. and Royston, Patrick},
  year = {2000},
  journal = {Statistics in Medicine},
  volume = {19},
  number = {4},
  pages = {453--473},
  issn = {1097-0258},
  doi = {10.1002/(SICI)1097-0258(20000229)19:4<453::AID-SIM350>3.0.CO;2-5},
  abstract = {Prognostic models are used in medicine for investigating patient outcome in relation to patient and disease characteristics. Such models do not always work well in practice, so it is widely recommended that they need to be validated. The idea of validating a prognostic model is generally taken to mean establishing that it works satisfactorily for patients other than those from whose data it was derived. In this paper we examine what is meant by validation and review why it is necessary. We consider how to validate a model and suggest that it is desirable to consider two rather different aspects \textendash{} statistical and clinical validity \textendash{} and examine some general approaches to validation. We illustrate the issues using several case studies. Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  copyright = {Copyright \textcopyright{} 2000 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/VC488NHE/Altman_Royston_2000_What do we mean by validating a prognostic model.pdf;/Users/christopherboyer/Zotero/storage/N6DRNIF2/(SICI)1097-0258(20000229)194453AID-SIM3503.0.html}
}

@article{bickel_discriminative_2009,
  title = {Discriminative {{Learning Under Covariate Shift}}},
  author = {Bickel, Steffen and Br{\"u}ckner, Michael and Scheffer, Tobias},
  year = {2009},
  month = dec,
  journal = {J. Mach. Learn. Res.},
  volume = {10},
  pages = {2137--2155},
  issn = {1532-4435},
  abstract = {We address classification problems for which the training instances are governed by an input distribution that is allowed to differ arbitrarily from the test distribution---problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. The problem of learning under covariate shift can be written as an integrated optimization problem. Instantiating the general optimization problem leads to a kernel logistic regression and an exponential model classifier for covariate shift. The optimization problem is convex under certain conditions; our findings also clarify the relationship to the known kernel mean matching procedure. We report on experiments on problems of spam filtering, text classification, and landmine detection.},
  file = {/Users/christopherboyer/Zotero/storage/VCG7XND8/Bickel et al. - 2009 - Discriminative Learning Under Covariate Shift.pdf}
}

@article{bild_multi-ethnic_2002,
  title = {Multi-{{Ethnic Study}} of {{Atherosclerosis}}: {{Objectives}} and {{Design}}},
  shorttitle = {Multi-{{Ethnic Study}} of {{Atherosclerosis}}},
  author = {Bild, Diane E. and Bluemke, David A. and Burke, Gregory L. and Detrano, Robert and Diez Roux, Ana V. and Folsom, Aaron R. and Greenland, Philip and JacobsJr., David R. and Kronmal, Richard and Liu, Kiang and Nelson, Jennifer Clark and O'Leary, Daniel and Saad, Mohammed F. and Shea, Steven and Szklo, Moyses and Tracy, Russell P.},
  year = {2002},
  month = nov,
  journal = {American Journal of Epidemiology},
  volume = {156},
  number = {9},
  pages = {871--881},
  issn = {0002-9262},
  doi = {10.1093/aje/kwf113},
  abstract = {The Multi-Ethnic Study of Atherosclerosis was initiated in July 2000 to investigate the prevalence, correlates, and progression of subclinical cardiovascular disease (CVD) in a population-based sample of 6,500 men and women aged 45\textendash 84 years. The cohort will be selected from six US field centers. Approximately 38\% of the cohort will be White, 28\% African-American, 23\% Hispanic, and 11\% Asian (of Chinese descent). Baseline measurements will include measurement of coronary calcium using computed tomography; measurement of ventricular mass and function using cardiac magnetic resonance imaging; measurement of flow-mediated brachial artery endothelial vasodilation, carotid intimal-medial wall thickness, and distensibility of the carotid arteries using ultrasonography; measurement of peripheral vascular disease using ankle and brachial blood pressures; electrocardiography; and assessments of microalbuminuria, standard CVD risk factors, sociodemographic factors, life habits, and psychosocial factors. Blood samples will be assayed for putative biochemical risk factors and stored for use in nested case-control studies. DNA will be extracted and lymphocytes will be immortalized for genetic studies. Measurement of selected subclinical disease indicators and risk factors will be repeated for the study of progression over 7 years. Participants will be followed through 2008 for identification and characterization of CVD events, including acute myocardial infarction and other coronary heart disease, stroke, peripheral vascular disease, and congestive heart failure; therapeutic interventions for CVD; and mortality.},
  file = {/Users/christopherboyer/Zotero/storage/HI8Z8UM5/Bild et al. - 2002 - Multi-Ethnic Study of Atherosclerosis Objectives .pdf;/Users/christopherboyer/Zotero/storage/PN3LUGW2/255904.html}
}

@article{brier_verification_1950,
  title = {{{VERIFICATION OF FORECASTS EXPRESSED IN TERMS OF PROBABILITY}}},
  author = {Brier, Glenn W.},
  year = {1950},
  month = jan,
  journal = {Mon. Wea. Rev.},
  volume = {78},
  number = {1},
  pages = {1--3},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/59XLUUPQ/Brier_1950_VERIFICATION OF FORECASTS EXPRESSED IN TERMS OF PROBABILITY.pdf;/Users/christopherboyer/Zotero/storage/GR5LU8GU/VERIFICATION-OF-FORECASTS-EXPRESSED-IN-TERMS-OF.html}
}

@article{chernozhukov_doubledebiased_2018,
  title = {Double/Debiased Machine Learning for Treatment and Structural Parameters},
  author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
  year = {2018},
  month = feb,
  journal = {Econom J},
  volume = {21},
  number = {1},
  pages = {C1-C68},
  publisher = {{Oxford Academic}},
  issn = {1368-4221},
  doi = {10.1111/ectj.12097},
  abstract = {Summary.  We revisit the classic semi-parametric problem of inference on a low-dimensional parameter \texttheta 0 in the presence of high-dimensional nuisance parameters},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/4HFXNX6R/Chernozhukov et al_2018_Double-debiased machine learning for treatment and structural parameters.pdf}
}

@misc{cui_semiparametric_2022,
  title = {Semiparametric Proximal Causal Inference},
  author = {Cui, Yifan and Pu, Hongming and Shi, Xu and Miao, Wang and Tchetgen, Eric Tchetgen},
  year = {2022},
  month = aug,
  number = {arXiv:2011.08411},
  eprint = {arXiv:2011.08411},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2011.08411},
  abstract = {Skepticism about the assumption of no unmeasured confounding, also known as exchangeability, is often warranted in making causal inferences from observational data; because exchangeability hinges on an investigator's ability to accurately measure covariates that capture all potential sources of confounding. In practice, the most one can hope for is that covariate measurements are at best proxies of the true underlying confounding mechanism operating in a given observational study. In this paper, we consider the framework of proximal causal inference introduced by Tchetgen Tchetgen et al. (2020), which while explicitly acknowledging covariate measurements as imperfect proxies of confounding mechanisms, offers an opportunity to learn about causal effects in settings where exchangeability on the basis of measured covariates fails. We make a number of contributions to proximal inference including (i) an alternative set of conditions for nonparametric proximal identification of the average treatment effect; (ii) general semiparametric theory for proximal estimation of the average treatment effect including efficiency bounds for key semiparametric models of interest; (iii) a characterization of proximal doubly robust and locally efficient estimators of the average treatment effect. Moreover, we provide analogous identification and efficiency results for the average treatment effect on the treated. Our approach is illustrated via simulation studies and a data application on evaluating the effectiveness of right heart catheterization in the intensive care unit of critically ill patients.},
  archiveprefix = {arxiv},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/Users/christopherboyer/Zotero/storage/NXFMUBDN/Cui et al. - 2022 - Semiparametric proximal causal inference.pdf;/Users/christopherboyer/Zotero/storage/RVMQY746/2011.html}
}

@article{dagostino_relation_1990,
  title = {Relation of Pooled Logistic Regression to Time Dependent Cox Regression Analysis: {{The}} Framingham Heart Study},
  shorttitle = {Relation of Pooled Logistic Regression to Time Dependent Cox Regression Analysis},
  author = {D'Agostino, Ralph B. and Lee, Mei-Ling and Belanger, Albert J. and Cupples, L. Adrienne and Anderson, Keaven and Kannel, William B.},
  year = {1990},
  journal = {Statistics in Medicine},
  volume = {9},
  number = {12},
  pages = {1501--1515},
  issn = {1097-0258},
  doi = {10.1002/sim.4780091214},
  abstract = {A standard analysis of the Framingham Heart Study data is a generalized person-years approach in which risk factors or covariates are measured every two years with a follow-up between these measurement times to observe the occurrence of events such as cardiovascular disease. Observations over multiple intervals are pooled into a single sample and a logistic regression is employed to relate the risk factors to the occurrence of the event. We show that this pooled logistic regression is close to the time dependent covariate Cox regression analysis. Numerical examples covering a variety of sample sizes and proportions of events display the closeness of this relationship in situations typical of the Framingham Study. A proof of the relationship and the necessary conditions are given in the Appendix.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/AVFMXVGK/D'Agostino et al. - 1990 - Relation of pooled logistic regression to time dep.pdf;/Users/christopherboyer/Zotero/storage/GSPJ466E/sim.html}
}

@article{dickerman_counterfactual_2020,
  title = {Counterfactual Prediction Is Not Only for Causal Inference},
  author = {Dickerman, Barbra A. and Hern{\'a}n, Miguel A.},
  year = {2020},
  month = jul,
  journal = {Eur J Epidemiol},
  volume = {35},
  number = {7},
  pages = {615--617},
  issn = {1573-7284},
  doi = {10.1007/s10654-020-00659-8},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/3X3WISPZ/Dickerman_Hernán_2020_Counterfactual prediction is not only for causal inference.pdf}
}

@article{dickerman_predicting_2022,
  title = {Predicting Counterfactual Risks under Hypothetical Treatment Strategies: An Application to {{HIV}}},
  shorttitle = {Predicting Counterfactual Risks under Hypothetical Treatment Strategies},
  author = {Dickerman, Barbra A. and Dahabreh, Issa J. and Cantos, Krystal V. and Logan, Roger W. and Lodi, Sara and Rentsch, Christopher T. and Justice, Amy C. and Hern{\'a}n, Miguel A.},
  year = {2022},
  month = apr,
  journal = {Eur J Epidemiol},
  volume = {37},
  number = {4},
  pages = {367--376},
  issn = {0393-2990, 1573-7284},
  doi = {10.1007/s10654-022-00855-8},
  abstract = {The accuracy of a prediction algorithm depends on contextual factors that may vary across deployment settings. To address this inherent limitation of prediction, we propose an approach to counterfactual prediction based on the g-formula to predict risk across populations that differ in their distribution of treatment strategies. We apply this to predict 5-year risk of mortality among persons receiving care for HIV in the U.S. Veterans Health Administration under different hypothetical treatment strategies. First, we implement a conventional approach to develop a prediction algorithm in the observed data and show how the algorithm may fail when transported to new populations with different treatment strategies. Second, we generate counterfactual data under different treatment strategies and use it to assess the robustness of the original algorithm's performance to these differences and to develop counterfactual prediction algorithms. We discuss how estimating counterfactual risks under a particular treatment strategy is more challenging than conventional prediction as it requires the same data, methods, and unverifiable assumptions as causal inference. However, this may be required when the alternative assumption of constant treatment patterns across deployment settings is unlikely to hold and new data is not yet available to retrain the algorithm.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/HV4HGW2W/Dickerman et al. - 2022 - Predicting counterfactual risks under hypothetical.pdf}
}

@article{efron_prediction_2020,
  title = {Prediction, {{Estimation}}, and {{Attribution}}},
  author = {Efron, Bradley},
  year = {2020},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {115},
  number = {530},
  pages = {636--655},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2020.1762613},
  abstract = {The scientific needs and computational limitations of the twentieth century fashioned classical statistical methodology. Both the needs and limitations have changed in the twenty-first, and so has the methodology. Large-scale prediction algorithms\textemdash neural nets, deep learning, boosting, support vector machines, random forests\textemdash have achieved star status in the popular press. They are recognizable as heirs to the regression tradition, but ones carried out at enormous scale and on titanic datasets. How do these algorithms compare with standard regression techniques such as ordinary least squares or logistic regression? Several key discrepancies will be examined, centering on the differences between prediction and estimation or prediction and attribution (significance testing). Most of the discussion is carried out through small numerical examples.},
  keywords = {Black box,Ephemeral predictors,Random forests,Surface plus noise},
  file = {/Users/christopherboyer/Zotero/storage/MCYLCFA4/Efron - 2020 - Prediction, Estimation, and Attribution.pdf;/Users/christopherboyer/Zotero/storage/KQJHCULL/01621459.2020.html}
}

@article{finlayson_clinician_2021,
  title = {The {{Clinician}} and {{Dataset Shift}} in {{Artificial Intelligence}}},
  author = {Finlayson, Samuel G. and Subbaswamy, Adarsh and Singh, Karandeep and Bowers, John and Kupke, Annabel and Zittrain, Jonathan and Kohane, Isaac S. and Saria, Suchi},
  year = {2021},
  month = jul,
  journal = {N Engl J Med},
  volume = {385},
  number = {3},
  pages = {283--286},
  publisher = {{Massachusetts Medical Society}},
  issn = {0028-4793},
  doi = {10.1056/NEJMc2104626},
  file = {/Users/christopherboyer/Zotero/storage/G9AABEHL/Finlayson et al. - 2021 - The Clinician and Dataset Shift in Artificial Inte.pdf}
}

@article{grundy_scott_m_2018_2019,
  title = {2018 {{AHA}}/{{ACC}}/{{AACVPR}}/{{AAPA}}/{{ABC}}/{{ACPM}}/{{ADA}}/{{AGS}} /{{APhA}}/{{ASPC}}/{{NLA}}/{{PCNA Guideline}} on the {{Management}} of {{Blood Cholesterol}}: {{A Report}} of the {{American College}} of {{Cardiology}}/{{American Heart Association Task Force}} on {{Clinical Practice Guidelines}}},
  shorttitle = {2018 {{AHA}}/{{ACC}}/{{AACVPR}}/{{AAPA}}/{{ABC}}/{{ACPM}}/{{ADA}}/{{AGS}}/{{APhA}}/{{ASPC}}/{{NLA}}/{{PCNA Guideline}} on the {{Management}} of {{Blood Cholesterol}}},
  author = {{Grundy Scott M.} and {Stone Neil J.} and {Bailey Alison L.} and {Beam Craig} and {Birtcher Kim K.} and {Blumenthal Roger S.} and {Braun Lynne T.} and {de Ferranti Sarah} and {Faiella-Tommasino Joseph} and {Forman Daniel E.} and {Goldberg Ronald} and {Heidenreich Paul A.} and {Hlatky Mark A.} and {Jones Daniel W.} and {Lloyd-Jones Donald} and {Lopez-Pajares Nuria} and {Ndumele Chiadi E.} and {Orringer Carl E.} and {Peralta Carmen A.} and {Saseen Joseph J.} and {Smith Sidney C.} and {Sperling Laurence} and {Virani Salim S.} and {Yeboah Joseph}},
  year = {2019},
  month = jun,
  journal = {Circulation},
  volume = {139},
  number = {25},
  pages = {e1082-e1143},
  publisher = {{American Heart Association}},
  doi = {10.1161/CIR.0000000000000625},
  file = {/Users/christopherboyer/Zotero/storage/3R2ECT5I/Grundy Scott M. et al. - 2019 - 2018 AHAACCAACVPRAAPAABCACPMADAAGSAPhAASP.pdf;/Users/christopherboyer/Zotero/storage/KUK7GXDL/CIR.html}
}

@article{harrell_multivariable_1996,
  title = {Multivariable {{Prognostic Models}}: {{Issues}} in {{Developing Models}}, {{Evaluating Assumptions}} and {{Adequacy}}, and {{Measuring}} and {{Reducing Errors}}},
  shorttitle = {Multivariable {{Prognostic Models}}},
  author = {Harrell, Frank E. and Lee, Kerry L. and Mark, Daniel B.},
  year = {1996},
  journal = {Statistics in Medicine},
  volume = {15},
  number = {4},
  pages = {361--387},
  issn = {1097-0258},
  doi = {10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4},
  abstract = {Multivariable regression models are powerful tools that are used frequently in studies of clinical outcomes. These models can use a mixture of categorical and continuous variables and can handle partially observed (censored) responses. However, uncritical application of modelling techniques can result in models that poorly fit the dataset at hand, or, even more likely, inaccurately predict outcomes on new subjects. One must know how to measure qualities of a model's fit in order to avoid poorly fitted or overfitted models. Measurement of predictive accuracy can be difficult for survival time data in the presence of censoring. We discuss an easily interpretable index of predictive discrimination as well as methods for assessing calibration of predicted survival probabilities. Both types of predictive accuracy should be unbiasedly validated using bootstrapping or cross-validation, before using predictions in a new data series. We discuss some of the hazards of poorly fitted and overfitted regression models and present one modelling strategy that avoids many of the problems discussed. The methods described are applicable to all regression models, but are particularly needed for binary, ordinal, and time-to-event outcomes. Methods are illustrated with a survival analysis in prostate cancer using Cox regression.},
  copyright = {Copyright \textcopyright{} 1996 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/TS5KG7PV/Harrell et al_1996_Multivariable Prognostic Models.pdf;/Users/christopherboyer/Zotero/storage/5BELPKYX/(SICI)1097-0258(19960229)154361AID-SIM1683.0.html}
}

@book{hastie_elements_2009,
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H and Friedman, Jerome H},
  year = {2009},
  volume = {2},
  publisher = {{Springer}},
  file = {/Users/christopherboyer/Zotero/storage/AEVVIZIG/Hastie et al. - 2009 - The elements of statistical learning data mining,.pdf}
}

@book{hernan_causal_2020,
  title = {Causal {{Inference}}: {{What If}}},
  author = {Hern{\'a}n, Miguel A and Robins, James M},
  year = {2020},
  publisher = {{Chapman \& Hall/CRC}},
  address = {{Boca Raton}},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/SV2T525Y/Hernán and Robins - Causal Inference What If.pdf}
}

@article{hernan_instruments_2006,
  title = {Instruments for {{Causal Inference}}},
  author = {Hernan, Miguel A and Robins, James M},
  year = {2006},
  volume = {17},
  number = {4},
  abstract = {The use of instrumental variable (IV) methods is attractive because, even in the presence of unmeasured confounding, such methods may consistently estimate the average causal effect of an exposure on an outcome. However, for this consistent estimation to be achieved, several strong conditions must hold. We review the definition of an instrumental variable, describe the conditions required to obtain consistent estimates of causal effects, and explore their implications in the context of a recent application of the instrumental variables approach. We also present (1) a description of the connection between 4 causal models\textemdash{} counterfactuals, causal directed acyclic graphs, nonparametric structural equation models, and linear structural equation models\textemdash that have been used to describe instrumental variables methods; (2) a unified presentation of IV methods for the average causal effect in the study population through structural mean models; and (3) a discussion and new extensions of instrumental variables methods based on assumptions of monotonicity.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/GAM4I3EW/Hernan and Robins - 2006 - Instruments for Causal Inference.pdf}
}

@article{hernan_second_2019,
  title = {A {{Second Chance}} to {{Get Causal Inference Right}}: {{A Classification}} of {{Data Science Tasks}}},
  shorttitle = {A {{Second Chance}} to {{Get Causal Inference Right}}},
  author = {Hern{\'a}n, Miguel A. and Hsu, John and Healy, Brian},
  year = {2019},
  month = jan,
  journal = {CHANCE},
  volume = {32},
  number = {1},
  pages = {42--49},
  issn = {0933-2480, 1867-2280},
  doi = {10.1080/09332480.2019.1579578},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/ZJ3R52LQ/Hernán et al. - 2019 - A Second Chance to Get Causal Inference Right A C.pdf}
}

@article{kent_predictive_2020,
  title = {The {{Predictive Approaches}} to {{Treatment}} Effect {{Heterogeneity}} ({{PATH}}) {{Statement}}},
  author = {Kent, David M. and Paulus, Jessica K. and {van Klaveren}, David and D'Agostino, Ralph and Goodman, Steve and Hayward, Rodney and Ioannidis, John P.A. and {Patrick-Lake}, Bray and Morton, Sally and Pencina, Michael and Raman, Gowri and Ross, Joseph S. and Selker, Harry P. and Varadhan, Ravi and Vickers, Andrew and Wong, John B. and Steyerberg, Ewout W.},
  year = {2020},
  month = jan,
  journal = {Ann Intern Med},
  volume = {172},
  number = {1},
  pages = {35--45},
  publisher = {{American College of Physicians}},
  issn = {0003-4819},
  doi = {10.7326/M18-3667},
  file = {/Users/christopherboyer/Zotero/storage/98NN8QA4/Kent et al. - 2020 - The Predictive Approaches to Treatment effect Hete.pdf}
}

@article{li_estimating_2022,
  title = {Estimating the Area under the {{ROC}} Curve When Transporting a Prediction Model to a Target Population},
  author = {Li, Bing and Gatsonis, Constantine and Dahabreh, Issa J. and Steingrimsson, Jon A.},
  year = {2022},
  month = nov,
  journal = {Biometrics},
  pages = {biom.13796},
  issn = {0006-341X, 1541-0420},
  doi = {10.1111/biom.13796},
  abstract = {We propose methods for estimating the area under the receiver operating characteristic (ROC) curve (AUC) of a prediction model in a target population that differs from the source population that provided the data used for original model development. If covariates that are associated with model performance, as measured by the AUC, have a different distribution in the source and target populations, then AUC estimators that only use data from the source population will not reflect model performance in the target population. Here, we provide identification results for the AUC in the target population when outcome and covariate data are available from the sample of the source population, but only covariate data are available from the sample of the target population. In this setting, we propose three estimators for the AUC in the target population and show that they are consistent and asymptotically normal. We evaluate the finite-sample performance of the estimators using simulations and use them to estimate the AUC in a nationally representative target population from the National Health and Nutrition Examination Survey for a lung cancer risk prediction model developed using source population data from the National Lung Screening Trial.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/GDD4YP4N/Li et al. - 2022 - Estimating the area under the ROC curve when trans.pdf}
}

@article{lin_scoping_2021,
  title = {A Scoping Review of Causal Methods Enabling Predictions under Hypothetical Interventions},
  author = {Lin, Lijing and Sperrin, Matthew and Jenkins, David A. and Martin, Glen P. and Peek, Niels},
  year = {2021},
  month = feb,
  journal = {Diagn Progn Res},
  volume = {5},
  number = {1},
  pages = {3},
  issn = {2397-7523},
  doi = {10.1186/s41512-021-00092-9},
  abstract = {The methods with which prediction models are usually developed mean that neither the parameters nor the predictions should be interpreted causally. For many applications, this is perfectly acceptable. However, when prediction models are used to support decision making, there is often a need for predicting outcomes under hypothetical interventions.},
  langid = {english},
  keywords = {Causal inference,Clinical prediction models,Counterfactual prediction,Statistical modeling},
  file = {/Users/christopherboyer/Zotero/storage/TNA89ZL3/Lin et al. - 2021 - A scoping review of causal methods enabling predic.pdf}
}

@misc{morrison_robust_2022,
  title = {Robust {{Estimation}} of {{Loss-Based Measures}} of {{Model Performance}} under {{Covariate Shift}}},
  author = {Morrison, Samantha and Gatsonis, Constantine and Dahabreh, Issa J. and Li, Bing and Steingrimsson, Jon A.},
  year = {2022},
  month = oct,
  number = {arXiv:2210.01980},
  eprint = {arXiv:2210.01980},
  publisher = {{arXiv}},
  abstract = {We present methods for estimating loss-based measures of the performance of a prediction model in a target population that differs from the source population in which the model was developed, in settings where outcome and covariate data are available from the source population but only covariate data are available on a simple random sample from the target population. Prior work adjusting for differences between the two populations has used various weighting estimators with inverse odds or density ratio weights. Here, we develop more robust estimators for the target population risk (expected loss) that can be used with data-adaptive (e.g., machine learning-based) estimation of nuisance parameters. We examine the large-sample properties of the estimators and evaluate finite sample performance in simulations. Last, we apply the methods to data from lung cancer screening using nationally representative data from the National Health and Nutrition Examination Survey (NHANES) and extend our methods to account for the complex survey design of the NHANES.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Methodology},
  file = {/Users/christopherboyer/Zotero/storage/XN43N64I/Morrison et al. - 2022 - Robust Estimation of Loss-Based Measures of Model .pdf;/Users/christopherboyer/Zotero/storage/YS6CUI3A/2210.html}
}

@article{pajouheshnia_accounting_2017,
  title = {Accounting for Treatment Use When Validating a Prognostic Model: A Simulation Study},
  shorttitle = {Accounting for Treatment Use When Validating a Prognostic Model},
  author = {Pajouheshnia, Romin and Peelen, Linda M. and Moons, Karel G. M. and Reitsma, Johannes B. and Groenwold, Rolf H. H.},
  year = {2017},
  month = jul,
  journal = {BMC Med Res Methodol},
  volume = {17},
  number = {1},
  pages = {103},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0375-8},
  abstract = {Prognostic models often show poor performance when applied to independent validation data sets. We illustrate how treatment use in a validation set can affect measures of model performance and present the uses and limitations of available analytical methods to account for this using simulated data.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/Q9Y3J57K/Pajouheshnia et al_2017_Accounting for treatment use when validating a prognostic model.pdf}
}

@article{pajouheshnia_accounting_2017-1,
  title = {Accounting for Treatment Use When Validating a Prognostic Model: A Simulation Study},
  shorttitle = {Accounting for Treatment Use When Validating a Prognostic Model},
  author = {Pajouheshnia, Romin and Peelen, Linda M. and Moons, Karel G. M. and Reitsma, Johannes B. and Groenwold, Rolf H. H.},
  year = {2017},
  month = jul,
  journal = {BMC Medical Research Methodology},
  volume = {17},
  number = {1},
  pages = {103},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0375-8},
  abstract = {Prognostic models often show poor performance when applied to independent validation data sets. We illustrate how treatment use in a validation set can affect measures of model performance and present the uses and limitations of available analytical methods to account for this using simulated data.},
  keywords = {Inverse Probability Weighting (IPW),Model Discrimination,Prognostic Model,Untreated Risk,Validation Data Set},
  file = {/Users/christopherboyer/Zotero/storage/TQ8I2A6I/Pajouheshnia et al. - 2017 - Accounting for treatment use when validating a pro.pdf;/Users/christopherboyer/Zotero/storage/LWVQVI6A/s12874-017-0375-8.html}
}

@article{robins_estimation_1994,
  title = {Estimation of {{Regression Coefficients When Some Regressors Are Not Always Observed}}},
  author = {Robins, James M. and Rotnitzky, Andrea and Zhao, Lue Ping},
  year = {1994},
  journal = {Journal of the American Statistical Association},
  volume = {89},
  number = {427},
  eprint = {2290910},
  eprinttype = {jstor},
  pages = {846--866},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2290910},
  abstract = {In applied problems it is common to specify a model for the conditional mean of a response given a set of regressors. A subset of the regressors may be missing for some study subjects either by design or happenstance. In this article we propose a new class of semiparametric estimators, based on inverse probability weighted estimating equations, that are consistent for parameter vector {$\alpha$}\textsubscript{0} of the conditional mean model when the data are missing at random in the sense of Rubin and the missingness probabilities are either known or can be parametrically modeled. We show that the asymptotic variance of the optimal estimator in our class attains the semiparametric variance bound for the model by first showing that our estimation problem is a special case of the general problem of parameter estimation in an arbitrary semiparametric model in which the data are missing at random and the probability of observing complete data is bounded away from 0, and then deriving a representation for the efficient score, the semiparametric variance bound, and the influence function of any regular, asymptotically linear estimator in this more general estimation problem. Because the optimal estimator depends on the unknown probability law generating the data, we propose locally and globally adaptive semiparametric efficient estimators. We compare estimators in our class with previously proposed estimators. We show that each previous estimator is asymptotically equivalent to some, usually inefficient, estimator in our class. This equivalence is a consequence of a proposition stating that every regular asymptotic linear estimator of {$\alpha$}\textsubscript{0} is asymptotically equivalent to some estimator in our class. We compare various estimators in a small simulation study and offer some practical recommendations.},
  file = {/Users/christopherboyer/Zotero/storage/3XRXNXIK/Robins et al_1994_Estimation of Regression Coefficients When Some Regressors Are Not Always.pdf}
}

@article{robins_graphical_1987,
  title = {A Graphical Approach to the Identification and Estimation of Causal Parameters in Mortality Studies with Sustained Exposure Periods},
  author = {Robins, James},
  year = {1987},
  month = jan,
  journal = {Journal of Chronic Diseases},
  volume = {40},
  pages = {139S-161S},
  issn = {00219681},
  doi = {10.1016/S0021-9681(87)80018-8},
  abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, independent risk factors for death commonly determine subsequent exposure to the study agent. For example, in occupational mortality studies, date of termination of employment is both a determinant of subsequent exposure to the chemical agent under study (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When a risk factor determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure can underestimate the true effect of exposure on mortality, whether or not one adjusts for the risk factor in the analysis. This observation raises the question, "Which, if any, empirical population parameter can be causally interpreted as the true effect of exposure in observational mortality studies?" In answer, we offer a graphical approach to the identification and estimation of causal parameters in mortality studies with sustained exposure periods. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers using our approach and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all cause and lung cancer mortality, which standard methods failed to detect. The analytic approach introduced in this paper may be necessary to control bias in any epidemiologic study in which there exists a risk factor which both determines subsequent exposure and is determined by previous exposure to the agent under study.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/53G3EYXD/Robins - 1987 - A graphical approach to the identification and est.pdf}
}

@article{robins_higher_2008,
  title = {Higher Order Influence Functions and Minimax Estimation of Nonlinear Functionals},
  author = {Robins, James and Li, Lingling and Tchetgen, Eric and {van der Vaart}, Aad},
  year = {2008},
  month = jan,
  journal = {Probability and Statistics: Essays in Honor of David A. Freedman},
  volume = {2},
  pages = {335--422},
  publisher = {{Institute of Mathematical Statistics}},
  doi = {10.1214/193940307000000527},
  abstract = {{$<$}!-- *** Custom HTML *** --{$><$}p{$>$} We present a theory of point and interval estimation for nonlinear functionals in parametric, semi-, and non-parametric models based on higher order influence functions (Robins (2004), Section 9; Li et al. (2004), Tchetgen et al. (2006), Robins et al. (2007)). Higher order influence functions are higher order U-statistics. Our theory extends the first order semiparametric theory of Bickel et al. (1993) and van der Vaart (1991) by incorporating the theory of higher order scores considered by Pfanzagl (1990), Small and McLeish (1994) and Lindsay and Waterman (1996). The theory reproduces many previous results, produces new non-\$\textbackslash sqrt\{n\}\$ results, and opens up the ability to perform optimal non-\$\textbackslash sqrt\{n\}\$ inference in complex high dimensional models. We present novel rate-optimal point and interval estimators for various functionals of central importance to biostatistics in settings in which estimation at the expected \$\textbackslash sqrt\{n\}\$ rate is not possible, owing to the curse of dimensionality. We also show that our higher order influence functions have a multi-robustness property that extends the double robustness property of first order influence functions described by Robins and Rotnitzky (2001) and van der Laan and Robins (2003). {$<$}/p{$>$}},
  file = {/Users/christopherboyer/Zotero/storage/FFPC4U34/Robins et al. - 2008 - Higher order influence functions and minimax estim.pdf}
}

@article{robins_new_1986,
  title = {A New Approach to Causal Inference in Mortality Studies with a Sustained Exposure Period\textemdash Application to Control of the Healthy Worker Survivor Effect},
  author = {Robins, James},
  year = {1986},
  month = jan,
  journal = {Mathematical Modelling},
  volume = {7},
  number = {9},
  pages = {1393--1512},
  issn = {0270-0255},
  doi = {10.1016/0270-0255(86)90088-6},
  abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject's assigned treatment protocol has been erased from the data file. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers with our method and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all-cause and lung cancer mortality which standard methods fail to detect.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/CLIME2RP/Robins_1986_A new approach to causal inference in mortality studies with a sustained.pdf;/Users/christopherboyer/Zotero/storage/Q5ZA6LD4/0270025586900886.html}
}

@inproceedings{robins_sensitivity_2000,
  title = {Sensitivity {{Analysis}} for {{Selection}} Bias and Unmeasured {{Confounding}} in Missing {{Data}} and {{Causal}} Inference Models},
  booktitle = {Statistical {{Models}} in {{Epidemiology}}, the {{Environment}}, and {{Clinical Trials}}},
  author = {Robins, James M. and Rotnitzky, Andrea and Scharfstein, Daniel O.},
  editor = {Halloran, M. Elizabeth and Berry, Donald},
  year = {2000},
  series = {The {{IMA Volumes}} in {{Mathematics}} and Its {{Applications}}},
  pages = {1--94},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-1284-3_1},
  abstract = {In both observational and randomized studies, subjects commonly drop out of the study (i.e., become censored) before end of follow-up. If, conditional on the history of the observed data up to t, the hazard of dropping out of the study (i.e., censoring) at time t does not depend on the possibly unobserved data subsequent to t, we say drop-out is ignorable or explainable (Rubin, 1976). On the other hand, if the hazard of drop-out depends on the possibly unobserved future, we say drop-out is non-ignorable or, equivalently, that there is selection bias on unobservables. Neither the existence of selection bias on unobservables nor its magnitude is identifiable from the joint distribution of the observables. In view of this fact, we argue that the data analyst should conduct a ``sensitivity analysis'' to quantify how one's inference concerning an outcome of interest varies as a function of the magnitude of non-identifiable selection bias.},
  isbn = {978-1-4612-1284-3},
  langid = {english},
  keywords = {Consistency Assumption,Current Status Data,Influence Function,Marginal Structural Model,Semiparametric Model},
  file = {/Users/christopherboyer/Zotero/storage/HUG7Z4C2/Robins et al. - 2000 - Sensitivity Analysis for Selection bias and unmeas.pdf}
}

@article{rolling2014model,
  title = {Model Selection for Estimating Treatment Effects},
  author = {Rolling, Craig A and Yang, Yuhong},
  year = {2014},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {76},
  number = {4},
  pages = {749--769},
  publisher = {{Wiley Online Library}},
  file = {/Users/christopherboyer/Zotero/storage/L2TDHEBS/Rolling and Yang - 2014 - Model selection for estimating treatment effects.pdf}
}

@inproceedings{schulam_reliable_2017-1,
  title = {Reliable {{Decision Support}} Using {{Counterfactual Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schulam, Peter and Saria, Suchi},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  abstract = {Decision-makers are faced with the challenge of estimating what is likely to happen when they take an action. For instance, if I choose not to treat this patient, are they likely to die? Practitioners commonly use supervised learning algorithms to fit predictive models that help decision-makers reason about likely future outcomes, but we show that this approach is unreliable, and sometimes even dangerous. The key issue is that supervised learning algorithms are highly sensitive to the policy used to choose actions in the training data, which causes the model to capture relationships that do not generalize. We propose using a different learning objective that predicts counterfactuals instead of predicting outcomes under an existing action policy as in supervised learning. To support decision-making in temporal settings, we introduce the Counterfactual Gaussian Process (CGP) to predict the counterfactual future progression of continuous-time trajectories under sequences of future actions. We demonstrate the benefits of the CGP on two important decision-support tasks: risk prediction and ``what if?'' reasoning for individualized treatment planning.},
  file = {/Users/christopherboyer/Zotero/storage/JWXI7RUF/Schulam and Saria - 2017 - Reliable Decision Support using Counterfactual Mod.pdf}
}

@misc{schuler_comparison_2018,
  title = {A Comparison of Methods for Model Selection When Estimating Individual Treatment Effects},
  author = {Schuler, Alejandro and Baiocchi, Michael and Tibshirani, Robert and Shah, Nigam},
  year = {2018},
  month = jun,
  number = {arXiv:1804.05146},
  eprint = {arXiv:1804.05146},
  publisher = {{arXiv}},
  abstract = {Practitioners in medicine, business, political science, and other fields are increasingly aware that decisions should be personalized to each patient, customer, or voter. A given treatment (e.g. a drug or advertisement) should be administered only to those who will respond most positively, and certainly not to those who will be harmed by it. Individual-level treatment effects can be estimated with tools adapted from machine learning, but different models can yield contradictory estimates. Unlike risk prediction models, however, treatment effect models cannot be easily evaluated against each other using a held-out test set because the true treatment effect itself is never directly observed. Besides outcome prediction accuracy, several metrics that can leverage held-out data to evaluate treatment effects models have been proposed, but they are not widely used. We provide a didactic framework that elucidates the relationships between the different approaches and compare them all using a variety of simulations of both randomized and observational data. Our results show that researchers estimating heterogenous treatment effects need not limit themselves to a single model-fitting algorithm. Instead of relying on a single method, multiple models fit by a diverse set of algorithms should be evaluated against each other using an objective function learned from the validation set. The model minimizing that objective should be used for estimating the individual treatment effect for future individuals.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/christopherboyer/Zotero/storage/VTWMTN5E/Schuler et al. - 2018 - A comparison of methods for model selection when e.pdf;/Users/christopherboyer/Zotero/storage/U7DKNPLU/1804.html}
}

@article{sperrin_using_2018,
  title = {Using Marginal Structural Models to Adjust for Treatment Drop-in When Developing Clinical Prediction Models},
  author = {Sperrin, Matthew and Martin, Glen P. and Pate, Alexander and Staa, Tjeerd Van and Peek, Niels and Buchan, Iain},
  year = {2018},
  journal = {Statistics in Medicine},
  volume = {37},
  number = {28},
  pages = {4142--4154},
  issn = {1097-0258},
  doi = {10.1002/sim.7913},
  abstract = {Clinical prediction models (CPMs) can inform decision making about treatment initiation, which requires predicted risks assuming no treatment is given. However, this is challenging since CPMs are usually derived using data sets where patients received treatment, often initiated postbaseline as ``treatment drop-ins.'' This study proposes the use of marginal structural models (MSMs) to adjust for treatment drop-in. We illustrate the use of MSMs in the CPM framework through simulation studies that represent randomized controlled trials and real-world observational data and the example of statin initiation for cardiovascular disease prevention. The simulations include a binary treatment and a covariate, each recorded at two timepoints and having a prognostic effect on a binary outcome. The bias in predicted risk was examined in a model ignoring treatment, a model fitted on treatment-na\"ive patients (at baseline), a model including baseline treatment, and the MSM. In all simulation scenarios, all models except the MSM underestimated the risk of outcome given absence of treatment. These results were supported in the statin initiation example, which showed that ignoring statin initiation postbaseline resulted in models that significantly underestimated the risk of a cardiovascular disease event occurring within 10 years. Consequently, CPMs that do not acknowledge treatment drop-in can lead to underallocation of treatment. In conclusion, when developing CPMs to predict treatment-na\"ive risk, researchers should consider using MSMs to adjust for treatment drop-in, and also seek to exploit the ability of MSMs to allow estimation of individual treatment effects.},
  langid = {english},
  keywords = {clinical prediction models,counterfactual causal inference,longitudinal data,marginal structural models,treatment drop-in,validation},
  file = {/Users/christopherboyer/Zotero/storage/Z34DXC8S/Sperrin et al_2018_Using marginal structural models to adjust for treatment drop-in when.pdf;/Users/christopherboyer/Zotero/storage/EN27BKBI/sim.html}
}

@article{stefanski_calculus_2002,
  title = {The {{Calculus}} of {{M-Estimation}}},
  author = {Stefanski, Leonard A and Boos, Dennis D},
  year = {2002},
  month = feb,
  journal = {The American Statistician},
  volume = {56},
  number = {1},
  pages = {29--38},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1198/000313002753631330},
  abstract = {Since the seminal papers by Huber in the 1960s, M-estimation methods (also known as estimating equation methods) have been increasingly important for asymptotic analysis and approximate inference. This article illustrates the breadth and generality of the M-estimation approach, thereby facilitating its use inpractice and in the classroom as a unifying approach to the study of large-sample inference.},
  keywords = {Asymptotic variance,Central limit theorem,Estimating equations,Large-sample inference,M-estimator,Maple},
  file = {/Users/christopherboyer/Zotero/storage/GTI5PSBY/Stefanski and Boos - 2002 - The Calculus of M-Estimation.pdf}
}

@article{steingrimsson_extending_2022,
  title = {Extending Prediction Models for Use in a New Target Population with Failure Time Outcomes},
  author = {Steingrimsson, Jon A},
  year = {2022},
  month = apr,
  journal = {Biostatistics},
  pages = {kxac011},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxac011},
  abstract = {Prediction models are often built and evaluated using data from a population that differs from the target population where model-derived predictions are intended to be used in. In this article, we present methods for evaluating model performance in the target population when some observations are right censored. The methods assume that outcome and covariate data are available from a source population used for model development and covariates, but no outcome data, are available from the target population. We evaluate the finite sample performance of the proposed estimators using simulations and apply the methods to transport a prediction model built using data from a lung cancer screening trial to a nationally representative population of participants eligible for lung cancer screening.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/JCS39J6N/Steingrimsson - 2022 - Extending prediction models for use in a new targe.pdf}
}

@misc{steingrimsson_transporting_2021,
  title = {Transporting a Prediction Model for Use in a New Target Population},
  author = {Steingrimsson, Jon A. and Gatsonis, Constantine and Dahabreh, Issa J.},
  year = {2021},
  month = apr,
  number = {arXiv:2101.11182},
  eprint = {arXiv:2101.11182},
  publisher = {{arXiv}},
  abstract = {We consider methods for transporting a prediction model and assessing its performance for use in a new target population, when outcome and covariate information for model development is available from a simple random sample from the source population, but only covariate information is available on a simple random sample from the target population. We discuss how to tailor the prediction model for use in the target population, how to assess model performance in the target population (e.g., by estimating the target population mean squared error), and how to perform model and tuning parameter selection in the context of the target population. We provide identifiability results for the target population mean squared error of a potentially misspecified prediction model under a sampling design where the source study and the target population samples are obtained separately. We also introduce the concept of prediction error modifiers that can be used to reason about the need for tailoring measures of model performance to the target population and provide an illustration of the methods using simulated data.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Applications},
  file = {/Users/christopherboyer/Zotero/storage/2IAXZKVU/Steingrimsson et al. - 2021 - Transporting a prediction model for use in a new t.pdf;/Users/christopherboyer/Zotero/storage/LPJGGJPX/2101.html}
}

@article{steyerberg_assessing_2010,
  title = {Assessing the {{Performance}} of {{Prediction Models}}: {{A Framework}} for {{Traditional}} and {{Novel Measures}}},
  shorttitle = {Assessing the {{Performance}} of {{Prediction Models}}},
  author = {Steyerberg, Ewout W. and Vickers, Andrew J. and Cook, Nancy R. and Gerds, Thomas and Gonen, Mithat and Obuchowski, Nancy and Pencina, Michael J. and Kattan, Michael W.},
  year = {2010},
  journal = {Epidemiology},
  volume = {21},
  number = {1},
  eprint = {25662818},
  eprinttype = {jstor},
  pages = {128--138},
  publisher = {{Lippincott Williams \& Wilkins}},
  issn = {1044-3983},
  abstract = {The performance of prediction models can be assessed using a variety of methods and metrics. Traditional measures for binary and survival outcomes include the Brier score to indicate overall model performance, the concordance (or c) statistic for discriminative ability (or area under the receiver operating characteristic [ROC] curve), and goodness-of-fit statistics for calibration. Several new measures have recently been proposed that can be seen as refinements of discrimination measures, including variants of the c statistic for survival, reclassification tables, net reclassification improvement (NRI), and integrated discrimination improvement (IDI). Moreover, decision\textemdash analytic measures have been proposed, including decision curves to plot the net benefit achieved by making decisions based on model predictions. We aimed to define the role of these relatively novel approaches in the evaluation of the performance of prediction models. For illustration, we present a case study of predicting the presence of residual tumor versus benign tissue in patients with testicular cancer (n = 544 for model development, n = 273 for external validation). We suggest that reporting discrimination and calibration will always be important for a prediction model. Decision-analytic measures should be reported if the predictive model is to be used for clinical decisions. Other measures of performance may be warranted in specific applications, such as reclassification metrics to gain insight into the value of adding a novel predictor to an established model.},
  file = {/Users/christopherboyer/Zotero/storage/K4K5HKJG/Steyerberg et al_2010_Assessing the Performance of Prediction Models.pdf}
}

@book{steyerberg_clinical_2019,
  title = {Clinical {{Prediction Models}}: {{A Practical Approach}} to {{Development}}, {{Validation}}, and {{Updating}}},
  shorttitle = {Clinical {{Prediction Models}}},
  author = {Steyerberg, Ewout W.},
  year = {2019},
  series = {Statistics for {{Biology}} and {{Health}}},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-16399-0},
  isbn = {978-3-030-16398-3 978-3-030-16399-0},
  langid = {english},
  file = {/Users/christopherboyer/Library/Mobile Documents/ZP9ZJ4EF3S~com~gingerlabs~Notability/Documents/2019_Book_ClinicalPredictionModels.nbn;/Users/christopherboyer/Zotero/storage/D9ZQ4QKU/Steyerberg - 2019 - Clinical Prediction Models A Practical Approach t.pdf}
}

@article{subbaswamy_development_2020,
  title = {From Development to Deployment: Dataset Shift, Causality, and Shift-Stable Models in Health {{AI}}},
  shorttitle = {From Development to Deployment},
  author = {Subbaswamy, Adarsh and Saria, Suchi},
  year = {2020},
  month = apr,
  journal = {Biostatistics},
  volume = {21},
  number = {2},
  pages = {345--352},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxz041},
  abstract = {The deployment of machine learning (ML) and statistical models is beginning to transform the practice of healthcare, with models now able to help clinicians diagnose conditions like pneumonia and skin cancer, and to predict which hospital patients are at risk of adverse events such as septic shock. A major concern, however, is that model performance is heavily tied to details particular to the dataset the model was developed on\textemdash even slight deviations from the training conditions can result in wildly different performance. For example, when researchers trained a model to diagnose pneumonia from chest X-rays using data from one health system, but evaluated on data from an external health system, they found the model performed significantly worse than it did internally (Zech and others, 2018). The model failed to generalize (i.e., predict accurately) due to the shifts between the training conditions (health system one) and the deployment/testing conditions (health system two). These shifts are very common when moving a model from the training phase to deployment and can take a variety of forms, including changes in patient demographics, disease prevalence, measurement timing, equipment, treatment patterns, and more. Beyond contributing to poor performance, failing to account for shifts can also lead to dangerous decisions in practice: the system can fail to diagnose severely ill patients or recommend harmful treatments. This problem of shifting conditions which prevent generalization is referred to as dataset shift (Qui\~nonero-Candela and others, 2009), and in this article, we explain what it is, why it occurs, give an overview of the types of existing solutions, and discuss open challenges that remain.},
  file = {/Users/christopherboyer/Zotero/storage/QCMXUZ45/Subbaswamy and Saria - 2020 - From development to deployment dataset shift, cau.pdf;/Users/christopherboyer/Zotero/storage/GBBXXELY/5631850.html}
}

@article{sugiyama_covariate_2007,
  title = {Covariate {{Shift Adaptation}} by {{Importance Weighted Cross Validation}}},
  author = {Sugiyama, Masashi and Krauledat, Matthias and M{\"u}ller, Klaus-Robert},
  year = {2007},
  journal = {Journal of Machine Learning Research},
  volume = {8},
  number = {35},
  pages = {985--1005},
  issn = {1533-7928},
  abstract = {A common assumption in supervised learning is that the input points in the training set follow the same probability distribution as the input points that will be given in the future test phase. However, this assumption is not satisfied, for example, when the outside of the training region is extrapolated. The situation where the training input points and test input points follow different distributions while the conditional distribution of output values given input points is unchanged is called the covariate shift. Under the covariate shift, standard model selection techniques such as cross validation do not work as desired since its unbiasedness is no longer maintained. In this paper, we propose a new method called importance weighted cross validation (IWCV), for which we prove its unbiasedness even under the covariate shift. The IWCV procedure is the only one that can be applied for unbiased classification under covariate shift, whereas alternatives to IWCV exist for regression. The usefulness of our proposed method is illustrated by simulations, and furthermore demonstrated in the brain-computer interface, where strong non-stationarity effects can be seen between training and test sessions.},
  file = {/Users/christopherboyer/Zotero/storage/DUY9D24N/Sugiyama et al. - 2007 - Covariate Shift Adaptation by Importance Weighted .pdf}
}

@misc{tchetgen_introduction_2020,
  title = {An {{Introduction}} to {{Proximal Causal Learning}}},
  author = {Tchetgen, Eric J. Tchetgen and Ying, Andrew and Cui, Yifan and Shi, Xu and Miao, Wang},
  year = {2020},
  month = sep,
  number = {arXiv:2009.10982},
  eprint = {arXiv:2009.10982},
  publisher = {{arXiv}},
  abstract = {A standard assumption for causal inference from observational data is that one has measured a sufficiently rich set of covariates to ensure that within covariate strata, subjects are exchangeable across observed treatment values. Skepticism about the exchangeability assumption in observational studies is often warranted because it hinges on investigators' ability to accurately measure covariates capturing all potential sources of confounding. Realistically, confounding mechanisms can rarely if ever, be learned with certainty from measured covariates. One can therefore only ever hope that covariate measurements are at best proxies of true underlying confounding mechanisms operating in an observational study, thus invalidating causal claims made on basis of standard exchangeability conditions. Causal learning from proxies is a challenging inverse problem which has to date remained unresolved. In this paper, we introduce a formal potential outcome framework for proximal causal learning, which while explicitly acknowledging covariate measurements as imperfect proxies of confounding mechanisms, offers an opportunity to learn about causal effects in settings where exchangeability on the basis of measured covariates fails. Sufficient conditions for nonparametric identification are given, leading to the proximal g-formula and corresponding proximal g-computation algorithm for estimation. These may be viewed as generalizations of Robins' foundational g-formula and g-computation algorithm, which account explicitly for bias due to unmeasured confounding. Both point treatment and time-varying treatment settings are considered, and an application of proximal g-computation of causal effects is given for illustration.},
  archiveprefix = {arxiv},
  keywords = {62A01,Statistics - Methodology},
  file = {/Users/christopherboyer/Zotero/storage/5E2UM395/Tchetgen et al. - 2020 - An Introduction to Proximal Causal Learning.pdf;/Users/christopherboyer/Zotero/storage/DNQT46XW/2009.html}
}

@article{van_geloven_prediction_2020,
  title = {Prediction Meets Causal Inference: The Role of Treatment in Clinical Prediction Models},
  shorttitle = {Prediction Meets Causal Inference},
  author = {{van Geloven}, Nan and Swanson, Sonja A. and Ramspek, Chava L. and Luijken, Kim and {van Diepen}, Merel and Morris, Tim P. and Groenwold, Rolf H. H. and {van Houwelingen}, Hans C. and Putter, Hein and {le Cessie}, Saskia},
  year = {2020},
  month = jul,
  journal = {Eur J Epidemiol},
  volume = {35},
  number = {7},
  pages = {619--630},
  issn = {1573-7284},
  doi = {10.1007/s10654-020-00636-1},
  abstract = {In this paper we study approaches for dealing with treatment when developing a clinical prediction model. Analogous to the estimand framework recently proposed by the European Medicines Agency for clinical trials, we propose a `predictimand' framework of different questions that may be of interest when predicting risk in relation to treatment started after baseline. We provide a formal definition of the estimands matching these questions, give examples of settings in which each is useful and discuss appropriate estimators including their assumptions. We illustrate the impact of the predictimand choice in a dataset of patients with end-stage kidney disease. We argue that clearly defining the estimand is equally important in prediction research as in causal inference.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/MRJTJ2XN/van Geloven et al_2020_Prediction meets causal inference.pdf}
}

@book{van2003unified,
  title = {Unified Methods for Censored Longitudinal Data and Causality},
  author = {{Van der Laan}, Mark J and Robins, James M},
  year = {2003},
  volume = {5},
  publisher = {{Springer}}
}

@misc{xu_calibration_2022,
  title = {Calibration {{Error}} for {{Heterogeneous Treatment Effects}}},
  author = {Xu, Yizhe and Yadlowsky, Steve},
  year = {2022},
  month = mar,
  number = {arXiv:2203.13364},
  eprint = {arXiv:2203.13364},
  publisher = {{arXiv}},
  abstract = {Recently, many researchers have advanced data-driven methods for modeling heterogeneous treatment effects (HTEs). Even still, estimation of HTEs is a difficult task -- these methods frequently over- or under-estimate the treatment effects, leading to poor calibration of the resulting models. However, while many methods exist for evaluating the calibration of prediction and classification models, formal approaches to assess the calibration of HTE models are limited to the calibration slope. In this paper, we define an analogue of the \textbackslash smash\{(\$\textbackslash ell\_2\$)\} expected calibration error for HTEs, and propose a robust estimator. Our approach is motivated by doubly robust treatment effect estimators, making it unbiased, and resilient to confounding, overfitting, and high-dimensionality issues. Furthermore, our method is straightforward to adapt to many structures under which treatment effects can be identified, including randomized trials, observational studies, and survival analysis. We illustrate how to use our proposed metric to evaluate the calibration of learned HTE models through the application to the CRITEO-UPLIFT Trial.},
  archiveprefix = {arxiv},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/christopherboyer/Zotero/storage/TBQ3NVJK/Xu and Yadlowsky - 2022 - Calibration Error for Heterogeneous Treatment Effe.pdf;/Users/christopherboyer/Zotero/storage/TSRIVP8P/2203.html}
}
