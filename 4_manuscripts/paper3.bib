@inproceedings{alaa_validating_2019,
  title = {Validating {{Causal Inference Models}} via {{Influence Functions}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Alaa, Ahmed and Schaar, Mihaela Van Der},
  date = {2019-05-24},
  pages = {191--201},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/alaa19a.html},
  urldate = {2022-12-15},
  abstract = {The problem of estimating causal effects of treatments from observational data falls beyond the realm of supervised learning \{—\} because counterfactual data is inaccessible, we can never observe the true causal effects. In the absence of "supervision", how can we evaluate the performance of causal inference methods? In this paper, we use influence functions \{—\} the functional derivatives of a loss function \{—\} to develop a model validation procedure that estimates the estimation error of causal inference methods. Our procedure utilizes a Taylor-like expansion to approximate the loss function of a method on a given dataset in terms of the influence functions of its loss on a "synthesized", proximal dataset with known causal effects. Under minimal regularity assumptions, we show that our procedure is consistent and efficient. Experiments on 77 benchmark datasets show that using our procedure, we can accurately predict the comparative performances of state-of-the-art causal inference methods applied to a given observational study.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/2UNYSEZK/Alaa and Schaar - 2019 - Validating Causal Inference Models via Influence F.pdf}
}

@article{altman_what_2000,
  title = {What Do We Mean by Validating a Prognostic Model?},
  author = {Altman, Douglas G. and Royston, Patrick},
  date = {2000},
  journaltitle = {Statistics in Medicine},
  volume = {19},
  number = {4},
  pages = {453--473},
  issn = {1097-0258},
  doi = {10.1002/(SICI)1097-0258(20000229)19:4<453::AID-SIM350>3.0.CO;2-5},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0258%2820000229%2919%3A4%3C453%3A%3AAID-SIM350%3E3.0.CO%3B2-5},
  urldate = {2020-11-12},
  abstract = {Prognostic models are used in medicine for investigating patient outcome in relation to patient and disease characteristics. Such models do not always work well in practice, so it is widely recommended that they need to be validated. The idea of validating a prognostic model is generally taken to mean establishing that it works satisfactorily for patients other than those from whose data it was derived. In this paper we examine what is meant by validation and review why it is necessary. We consider how to validate a model and suggest that it is desirable to consider two rather different aspects – statistical and clinical validity – and examine some general approaches to validation. We illustrate the issues using several case studies. Copyright © 2000 John Wiley \& Sons, Ltd.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/VC488NHE/Altman_Royston_2000_What do we mean by validating a prognostic model.pdf;/Users/christopherboyer/Zotero/storage/N6DRNIF2/(SICI)1097-0258(20000229)194453AID-SIM3503.0.html}
}

@inproceedings{bickel_discriminative_2007,
  title = {Discriminative Learning for Differing Training and Test Distributions},
  booktitle = {Proceedings of the 24th International Conference on {{Machine}} Learning},
  author = {Bickel, Steffen and Brückner, Michael and Scheffer, Tobias},
  date = {2007-06-20},
  series = {{{ICML}} '07},
  pages = {81--88},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1273496.1273507},
  url = {https://dl.acm.org/doi/10.1145/1273496.1273507},
  urldate = {2023-06-14},
  abstract = {We address classification problems for which the training instances are governed by a distribution that is allowed to differ arbitrarily from the test distribution---problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. We formulate the general problem of learning under covariate shift as an integrated optimization problem. We derive a kernel logistic regression classifier for differing training and test distributions.},
  isbn = {978-1-59593-793-3},
  file = {/Users/christopherboyer/Zotero/storage/68IQW6TW/Bickel et al. - 2007 - Discriminative learning for differing training and.pdf}
}

@article{bickel_discriminative_2009,
  title = {Discriminative {{Learning Under Covariate Shift}}},
  author = {Bickel, Steffen and Brückner, Michael and Scheffer, Tobias},
  date = {2009-12-01},
  journaltitle = {J. Mach. Learn. Res.},
  volume = {10},
  pages = {2137--2155},
  issn = {1532-4435},
  abstract = {We address classification problems for which the training instances are governed by an input distribution that is allowed to differ arbitrarily from the test distribution---problems also referred to as classification under covariate shift. We derive a solution that is purely discriminative: neither training nor test distribution are modeled explicitly. The problem of learning under covariate shift can be written as an integrated optimization problem. Instantiating the general optimization problem leads to a kernel logistic regression and an exponential model classifier for covariate shift. The optimization problem is convex under certain conditions; our findings also clarify the relationship to the known kernel mean matching procedure. We report on experiments on problems of spam filtering, text classification, and landmine detection.},
  file = {/Users/christopherboyer/Zotero/storage/VCG7XND8/Bickel et al. - 2009 - Discriminative Learning Under Covariate Shift.pdf}
}

@article{bild_multi-ethnic_2002,
  title = {Multi-{{Ethnic Study}} of {{Atherosclerosis}}: {{Objectives}} and {{Design}}},
  shorttitle = {Multi-{{Ethnic Study}} of {{Atherosclerosis}}},
  author = {Bild, Diane E. and Bluemke, David A. and Burke, Gregory L. and Detrano, Robert and Diez Roux, Ana V. and Folsom, Aaron R. and Greenland, Philip and JacobsJr., David R. and Kronmal, Richard and Liu, Kiang and Nelson, Jennifer Clark and O’Leary, Daniel and Saad, Mohammed F. and Shea, Steven and Szklo, Moyses and Tracy, Russell P.},
  date = {2002-11-01},
  journaltitle = {American Journal of Epidemiology},
  volume = {156},
  number = {9},
  pages = {871--881},
  issn = {0002-9262},
  doi = {10.1093/aje/kwf113},
  url = {https://doi.org/10.1093/aje/kwf113},
  urldate = {2022-09-15},
  abstract = {The Multi-Ethnic Study of Atherosclerosis was initiated in July 2000 to investigate the prevalence, correlates, and progression of subclinical cardiovascular disease (CVD) in a population-based sample of 6,500 men and women aged 45–84 years. The cohort will be selected from six US field centers. Approximately 38\% of the cohort will be White, 28\% African-American, 23\% Hispanic, and 11\% Asian (of Chinese descent). Baseline measurements will include measurement of coronary calcium using computed tomography; measurement of ventricular mass and function using cardiac magnetic resonance imaging; measurement of flow-mediated brachial artery endothelial vasodilation, carotid intimal-medial wall thickness, and distensibility of the carotid arteries using ultrasonography; measurement of peripheral vascular disease using ankle and brachial blood pressures; electrocardiography; and assessments of microalbuminuria, standard CVD risk factors, sociodemographic factors, life habits, and psychosocial factors. Blood samples will be assayed for putative biochemical risk factors and stored for use in nested case-control studies. DNA will be extracted and lymphocytes will be immortalized for genetic studies. Measurement of selected subclinical disease indicators and risk factors will be repeated for the study of progression over 7 years. Participants will be followed through 2008 for identification and characterization of CVD events, including acute myocardial infarction and other coronary heart disease, stroke, peripheral vascular disease, and congestive heart failure; therapeutic interventions for CVD; and mortality.},
  file = {/Users/christopherboyer/Zotero/storage/HI8Z8UM5/Bild et al. - 2002 - Multi-Ethnic Study of Atherosclerosis Objectives .pdf;/Users/christopherboyer/Zotero/storage/PN3LUGW2/255904.html}
}

@article{braitman_rare_2002,
  title = {Rare {{Outcomes}}, {{Common Treatments}}: {{Analytic Strategies Using Propensity Scores}}},
  shorttitle = {Rare {{Outcomes}}, {{Common Treatments}}},
  author = {Braitman, Leonard E. and Rosenbaum, Paul R.},
  date = {2002-10-15},
  journaltitle = {Ann Intern Med},
  volume = {137},
  number = {8},
  pages = {693--695},
  publisher = {{American College of Physicians}},
  issn = {0003-4819},
  doi = {10.7326/0003-4819-137-8-200210150-00015},
  url = {https://www.acpjournals.org/doi/abs/10.7326/0003-4819-137-8-200210150-00015},
  urldate = {2023-08-23}
}

@article{brier_verification_1950,
  title = {{{VERIFICATION OF FORECASTS EXPRESSED IN TERMS OF PROBABILITY}}},
  author = {Brier, Glenn W.},
  date = {1950-01-01},
  journaltitle = {Mon. Wea. Rev.},
  volume = {78},
  number = {1},
  pages = {1--3},
  publisher = {{American Meteorological Society}},
  issn = {0027-0644},
  doi = {10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2},
  url = {https://journals.ametsoc.org/mwr/article/78/1/1/96424/VERIFICATION-OF-FORECASTS-EXPRESSED-IN-TERMS-OF},
  urldate = {2020-11-12},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/59XLUUPQ/Brier_1950_VERIFICATION OF FORECASTS EXPRESSED IN TERMS OF PROBABILITY.pdf;/Users/christopherboyer/Zotero/storage/GR5LU8GU/VERIFICATION-OF-FORECASTS-EXPRESSED-IN-TERMS-OF.html}
}

@article{chernozhukov_doubledebiased_2018,
  title = {Double/Debiased Machine Learning for Treatment and Structural Parameters},
  author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
  date = {2018-02-01},
  journaltitle = {Econom J},
  volume = {21},
  number = {1},
  pages = {C1-C68},
  publisher = {{Oxford Academic}},
  issn = {1368-4221},
  doi = {10.1111/ectj.12097},
  url = {https://academic.oup.com/ectj/article/21/1/C1/5056401},
  urldate = {2020-11-12},
  abstract = {Summary.  We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/4HFXNX6R/Chernozhukov et al_2018_Double-debiased machine learning for treatment and structural parameters.pdf}
}

@online{coston_counterfactual_2020,
  title = {Counterfactual {{Risk Assessments}}, {{Evaluation}}, and {{Fairness}}},
  author = {Coston, Amanda and Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
  date = {2020-01-10},
  eprint = {1909.00066},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1909.00066},
  url = {http://arxiv.org/abs/1909.00066},
  urldate = {2023-09-06},
  abstract = {Algorithmic risk assessments are increasingly used to help humans make decisions in high-stakes settings, such as medicine, criminal justice and education. In each of these cases, the purpose of the risk assessment tool is to inform actions, such as medical treatments or release conditions, often with the aim of reducing the likelihood of an adverse event such as hospital readmission or recidivism. Problematically, most tools are trained and evaluated on historical data in which the outcomes observed depend on the historical decision-making policy. These tools thus reflect risk under the historical policy, rather than under the different decision options that the tool is intended to inform. Even when tools are constructed to predict risk under a specific decision, they are often improperly evaluated as predictors of the target outcome. Focusing on the evaluation task, in this paper we define counterfactual analogues of common predictive performance and algorithmic fairness metrics that we argue are better suited for the decision-making context. We introduce a new method for estimating the proposed metrics using doubly robust estimation. We provide theoretical results that show that only under strong conditions can fairness according to the standard metric and the counterfactual metric simultaneously hold. Consequently, fairness-promoting methods that target parity in a standard fairness metric may --- and as we show empirically, do --- induce greater imbalance in the counterfactual analogue. We provide empirical comparisons on both synthetic data and a real world child welfare dataset to demonstrate how the proposed method improves upon standard practice.},
  pubstate = {preprint},
  keywords = {Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/christopherboyer/Zotero/storage/T3X7TWFZ/Coston et al. - 2020 - Counterfactual Risk Assessments, Evaluation, and F.pdf;/Users/christopherboyer/Zotero/storage/4EA5TSZW/1909.html}
}

@online{coston_counterfactual_2021,
  title = {Counterfactual {{Predictions}} under {{Runtime Confounding}}},
  author = {Coston, Amanda and Kennedy, Edward H. and Chouldechova, Alexandra},
  date = {2021-04-15},
  eprint = {2006.16916},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/2006.16916},
  urldate = {2023-09-06},
  abstract = {Algorithms are commonly used to predict outcomes under a particular decision or intervention, such as predicting whether an offender will succeed on parole if placed under minimal supervision. Generally, to learn such counterfactual prediction models from observational data on historical decisions and corresponding outcomes, one must measure all factors that jointly affect the outcomes and the decision taken. Motivated by decision support applications, we study the counterfactual prediction task in the setting where all relevant factors are captured in the historical data, but it is either undesirable or impermissible to use some such factors in the prediction model. We refer to this setting as runtime confounding. We propose a doubly-robust procedure for learning counterfactual prediction models in this setting. Our theoretical analysis and experimental results suggest that our method often outperforms competing approaches. We also present a validation procedure for evaluating the performance of counterfactual prediction methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/christopherboyer/Zotero/storage/UD2B8AUU/Coston et al. - 2021 - Counterfactual Predictions under Runtime Confoundi.pdf;/Users/christopherboyer/Zotero/storage/T96FGSTW/2006.html}
}

@online{cui_semiparametric_2022,
  title = {Semiparametric Proximal Causal Inference},
  author = {Cui, Yifan and Pu, Hongming and Shi, Xu and Miao, Wang and Tchetgen, Eric Tchetgen},
  date = {2022-08-19},
  eprint = {2011.08411},
  eprinttype = {arxiv},
  eprintclass = {math, stat},
  doi = {10.48550/arXiv.2011.08411},
  url = {http://arxiv.org/abs/2011.08411},
  urldate = {2022-12-15},
  abstract = {Skepticism about the assumption of no unmeasured confounding, also known as exchangeability, is often warranted in making causal inferences from observational data; because exchangeability hinges on an investigator's ability to accurately measure covariates that capture all potential sources of confounding. In practice, the most one can hope for is that covariate measurements are at best proxies of the true underlying confounding mechanism operating in a given observational study. In this paper, we consider the framework of proximal causal inference introduced by Tchetgen Tchetgen et al. (2020), which while explicitly acknowledging covariate measurements as imperfect proxies of confounding mechanisms, offers an opportunity to learn about causal effects in settings where exchangeability on the basis of measured covariates fails. We make a number of contributions to proximal inference including (i) an alternative set of conditions for nonparametric proximal identification of the average treatment effect; (ii) general semiparametric theory for proximal estimation of the average treatment effect including efficiency bounds for key semiparametric models of interest; (iii) a characterization of proximal doubly robust and locally efficient estimators of the average treatment effect. Moreover, we provide analogous identification and efficiency results for the average treatment effect on the treated. Our approach is illustrated via simulation studies and a data application on evaluating the effectiveness of right heart catheterization in the intensive care unit of critically ill patients.},
  pubstate = {preprint},
  keywords = {Mathematics - Statistics Theory,Statistics - Methodology},
  file = {/Users/christopherboyer/Zotero/storage/NXFMUBDN/Cui et al. - 2022 - Semiparametric proximal causal inference.pdf;/Users/christopherboyer/Zotero/storage/RVMQY746/2011.html}
}

@article{dagostino_relation_1990,
  title = {Relation of Pooled Logistic Regression to Time Dependent Cox Regression Analysis: {{The}} Framingham Heart Study},
  shorttitle = {Relation of Pooled Logistic Regression to Time Dependent Cox Regression Analysis},
  author = {D'Agostino, Ralph B. and Lee, Mei-Ling and Belanger, Albert J. and Cupples, L. Adrienne and Anderson, Keaven and Kannel, William B.},
  date = {1990},
  journaltitle = {Statistics in Medicine},
  volume = {9},
  number = {12},
  pages = {1501--1515},
  issn = {1097-0258},
  doi = {10.1002/sim.4780091214},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/sim.4780091214},
  urldate = {2022-07-29},
  abstract = {A standard analysis of the Framingham Heart Study data is a generalized person-years approach in which risk factors or covariates are measured every two years with a follow-up between these measurement times to observe the occurrence of events such as cardiovascular disease. Observations over multiple intervals are pooled into a single sample and a logistic regression is employed to relate the risk factors to the occurrence of the event. We show that this pooled logistic regression is close to the time dependent covariate Cox regression analysis. Numerical examples covering a variety of sample sizes and proportions of events display the closeness of this relationship in situations typical of the Framingham Study. A proof of the relationship and the necessary conditions are given in the Appendix.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/AVFMXVGK/D'Agostino et al. - 1990 - Relation of pooled logistic regression to time dep.pdf;/Users/christopherboyer/Zotero/storage/GSPJ466E/sim.html}
}

@article{dahabreh_using_2016,
  title = {Using Group Data to Treat Individuals: Understanding Heterogeneous Treatment Effects in the Age of Precision Medicine and Patient-Centred Evidence},
  shorttitle = {Using Group Data to Treat Individuals},
  author = {Dahabreh, Issa J and Hayward, Rodney and Kent, David M},
  date = {2016-12-01},
  journaltitle = {International Journal of Epidemiology},
  volume = {45},
  number = {6},
  pages = {2184--2193},
  issn = {0300-5771},
  doi = {10.1093/ije/dyw125},
  url = {https://doi.org/10.1093/ije/dyw125},
  urldate = {2023-06-19},
  abstract = {Although often conflated, determining the best treatment for an individual (the task of a doctor) is fundamentally different from determining the average effect of treatment in a population (the purpose of a trial). In this paper, we review concepts of heterogeneity of treatment effects (HTE) essential in providing the evidence base for precision medicine and patient-centred care, and explore some inherent limitations of using group data (e.g. from a randomized trial) to guide treatment decisions for individuals. We distinguish between person-level HTE (i.e. that individuals experience different effects from a treatment) and group-level HTE (i.e. that subgroups have different average treatment effects), and discuss the reference class problem, engendered by the large number of potentially informative subgroupings of a study population (each of which may lead to applying a different estimated effect to the same patient), and the scale dependence of group-level HTE. We also review the limitations of conventional ‘one-variable-at-a-time’ subgroup analyses and discuss the potential benefits of using more comprehensive subgrouping schemes that incorporate information on multiple variables, such as those based on predicted outcome risk. Understanding the conceptual underpinnings of HTE is critical for understanding how studies can be designed, analysed, and interpreted to better inform individualized clinical decisions.},
  file = {/Users/christopherboyer/Zotero/storage/M5IRXHP9/Dahabreh et al. - 2016 - Using group data to treat individuals understandi.pdf;/Users/christopherboyer/Zotero/storage/4EYPKIFB/2670316.html}
}

@article{dickerman_counterfactual_2020,
  title = {Counterfactual Prediction Is Not Only for Causal Inference},
  author = {Dickerman, Barbra A. and Hernán, Miguel A.},
  date = {2020-07-01},
  journaltitle = {Eur J Epidemiol},
  volume = {35},
  number = {7},
  pages = {615--617},
  issn = {1573-7284},
  doi = {10.1007/s10654-020-00659-8},
  url = {https://doi.org/10.1007/s10654-020-00659-8},
  urldate = {2020-08-12},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/3X3WISPZ/Dickerman_Hernán_2020_Counterfactual prediction is not only for causal inference.pdf}
}

@article{dickerman_predicting_2022,
  title = {Predicting Counterfactual Risks under Hypothetical Treatment Strategies: An Application to {{HIV}}},
  shorttitle = {Predicting Counterfactual Risks under Hypothetical Treatment Strategies},
  author = {Dickerman, Barbra A. and Dahabreh, Issa J. and Cantos, Krystal V. and Logan, Roger W. and Lodi, Sara and Rentsch, Christopher T. and Justice, Amy C. and Hernán, Miguel A.},
  date = {2022-04},
  journaltitle = {Eur J Epidemiol},
  volume = {37},
  number = {4},
  pages = {367--376},
  issn = {0393-2990, 1573-7284},
  doi = {10.1007/s10654-022-00855-8},
  url = {https://link.springer.com/10.1007/s10654-022-00855-8},
  urldate = {2022-08-29},
  abstract = {The accuracy of a prediction algorithm depends on contextual factors that may vary across deployment settings. To address this inherent limitation of prediction, we propose an approach to counterfactual prediction based on the g-formula to predict risk across populations that differ in their distribution of treatment strategies. We apply this to predict 5-year risk of mortality among persons receiving care for HIV in the U.S. Veterans Health Administration under different hypothetical treatment strategies. First, we implement a conventional approach to develop a prediction algorithm in the observed data and show how the algorithm may fail when transported to new populations with different treatment strategies. Second, we generate counterfactual data under different treatment strategies and use it to assess the robustness of the original algorithm’s performance to these differences and to develop counterfactual prediction algorithms. We discuss how estimating counterfactual risks under a particular treatment strategy is more challenging than conventional prediction as it requires the same data, methods, and unverifiable assumptions as causal inference. However, this may be required when the alternative assumption of constant treatment patterns across deployment settings is unlikely to hold and new data is not yet available to retrain the algorithm.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/HV4HGW2W/Dickerman et al. - 2022 - Predicting counterfactual risks under hypothetical.pdf}
}

@article{efron_prediction_2020,
  title = {Prediction, {{Estimation}}, and {{Attribution}}},
  author = {Efron, Bradley},
  date = {2020-04-02},
  journaltitle = {Journal of the American Statistical Association},
  volume = {115},
  number = {530},
  pages = {636--655},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2020.1762613},
  url = {https://doi.org/10.1080/01621459.2020.1762613},
  urldate = {2021-03-08},
  abstract = {The scientific needs and computational limitations of the twentieth century fashioned classical statistical methodology. Both the needs and limitations have changed in the twenty-first, and so has the methodology. Large-scale prediction algorithms—neural nets, deep learning, boosting, support vector machines, random forests—have achieved star status in the popular press. They are recognizable as heirs to the regression tradition, but ones carried out at enormous scale and on titanic datasets. How do these algorithms compare with standard regression techniques such as ordinary least squares or logistic regression? Several key discrepancies will be examined, centering on the differences between prediction and estimation or prediction and attribution (significance testing). Most of the discussion is carried out through small numerical examples.},
  keywords = {Black box,Ephemeral predictors,Random forests,Surface plus noise},
  file = {/Users/christopherboyer/Zotero/storage/MCYLCFA4/Efron - 2020 - Prediction, Estimation, and Attribution.pdf;/Users/christopherboyer/Zotero/storage/KQJHCULL/01621459.2020.html}
}

@article{finlayson_clinician_2021,
  title = {The {{Clinician}} and {{Dataset Shift}} in {{Artificial Intelligence}}},
  author = {Finlayson, Samuel G. and Subbaswamy, Adarsh and Singh, Karandeep and Bowers, John and Kupke, Annabel and Zittrain, Jonathan and Kohane, Isaac S. and Saria, Suchi},
  date = {2021-07-15},
  journaltitle = {N Engl J Med},
  volume = {385},
  number = {3},
  pages = {283--286},
  publisher = {{Massachusetts Medical Society}},
  issn = {0028-4793},
  doi = {10.1056/NEJMc2104626},
  url = {https://www-nejm-org.ezp-prod1.hul.harvard.edu/doi/10.1056/NEJMc2104626},
  urldate = {2022-10-31},
  file = {/Users/christopherboyer/Zotero/storage/G9AABEHL/Finlayson et al. - 2021 - The Clinician and Dataset Shift in Artificial Inte.pdf}
}

@article{glasziou_evidence_1995,
  title = {An Evidence Based Approach to Individualising Treatment},
  author = {Glasziou, Paul P. and Irwig, Les M.},
  date = {1995-11-18},
  journaltitle = {BMJ},
  volume = {311},
  number = {7016},
  eprint = {7496291},
  eprinttype = {pmid},
  pages = {1356--1359},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {0959-8138, 1468-5833},
  doi = {10.1136/bmj.311.7016.1356},
  url = {https://www.bmj.com/content/311/7016/1356},
  urldate = {2023-06-19},
  abstract = {To which groups of patients can the results of clinical trials be applied? This question is often inappropriately answered by reference to the trial entry criteria. Instead, the benefit and harm (adverse events, discomfort of treatment, etc) of treatment could be assessed separately for individual patients. Patients at greatest risk of a disease will have the greatest net benefit as benefit to patients usually increases with risk while harm remains comparatively fixed. To assess net benefit, the relative risks should come from (a meta-analysis of) randomised trials; the risk in individual patients should come from multivariate risk equations derived from cohort studies. However, before making firm conclusions, the assumptions of fixed adverse effects and constant reduction in relative risk need to be checked. Should all patients with acute myocardial infarct receive streptokinase? Should all patients with non-valvar atrial fibrillation receive warfarin? Such questions are best answered by assessing benefits and risks in each patient rather than focusing on the inclusion and exclusion criteria of the trial. For example, patients with a history of peptic ulcer were often excluded from thrombolytic trials because it would be unethical to put such patients at risk of major bleeding episodes, given the lack of proved benefit. However, now that we know the size of the benefit--about three deaths saved for every 100 patients treated1--clinicians must make an informed decision that weighs this benefit against the potential for harm in patients with ulcers. Relying on the eligibility criteria for clinical trials is both erroneous and limiting. A too restrictive generalisation needs to be guarded against, and we are advised to ask, “Are the patients in this study so different from my patients that I could not apply the study results?”2 This is good advice, but how then do we decide when a …},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/8SFW4FUG/Glasziou and Irwig - 1995 - An evidence based approach to individualising trea.pdf;/Users/christopherboyer/Zotero/storage/HVSMS9DT/1356.full.html}
}

@article{groenwold_explicit_2016-1,
  title = {Explicit Inclusion of Treatment in Prognostic Modeling Was Recommended in Observational and Randomized Settings},
  author = {Groenwold, Rolf H. H. and Moons, Karel G. M. and Pajouheshnia, Romin and Altman, Doug G. and Collins, Gary S. and Debray, Thomas P. A. and Reitsma, Johannes B. and Riley, Richard D. and Peelen, Linda M.},
  date = {2016-10},
  journaltitle = {J Clin Epidemiol},
  volume = {78},
  eprint = {27045189},
  eprinttype = {pmid},
  pages = {90--100},
  issn = {1878-5921},
  doi = {10.1016/j.jclinepi.2016.03.017},
  abstract = {OBJECTIVES: To compare different methods to handle treatment when developing a prognostic model that aims to produce accurate probabilities of the outcome of individuals if left untreated. STUDY DESIGN AND SETTING: Simulations were performed based on two normally distributed predictors, a binary outcome, and a binary treatment, mimicking a randomized trial or an observational study. Comparison was made between simply ignoring treatment (SIT), restricting the analytical data set to untreated individuals (AUT), inverse probability weighting (IPW), and explicit modeling of treatment (MT). Methods were compared in terms of predictive performance of the model and the proportion of incorrect treatment decisions. RESULTS: Omitting a genuine predictor of the outcome from the prognostic model decreased model performance, in both an observational study and a randomized trial. In randomized trials, the proportion of incorrect treatment decisions was smaller when applying AUT or MT, compared to SIT and IPW. In observational studies, MT was superior to all other methods regarding the proportion of incorrect treatment decisions. CONCLUSION: If a prognostic model aims to produce correct probabilities of the outcome in the absence of treatment, ignoring treatments that affect that outcome can lead to suboptimal model performance and incorrect treatment decisions. Explicitly, modeling treatment is recommended.},
  langid = {english},
  keywords = {Calibration,Computer simulation,Computer Simulation,Decision support techniques,Epidemiologic Studies,Humans,Models,{Models, Statistical},Observational Studies as Topic,Probability,Prognosis,Randomized Controlled Trials as Topic,Statistical,Treatment Outcome},
  file = {/Users/christopherboyer/Zotero/storage/EATNQRP5/Groenwold et al. - 2016 - Explicit inclusion of treatment in prognostic mode.pdf}
}

@article{grundy_scott_m_2018_2019,
  title = {2018 {{AHA}}/{{ACC}}/{{AACVPR}}/{{AAPA}}/{{ABC}}/{{ACPM}}/{{ADA}}/{{AGS}} /{{APhA}}/{{ASPC}}/{{NLA}}/{{PCNA Guideline}} on the {{Management}} of {{Blood Cholesterol}}: {{A Report}} of the {{American College}} of {{Cardiology}}/{{American Heart Association Task Force}} on {{Clinical Practice Guidelines}}},
  shorttitle = {2018 {{AHA}}/{{ACC}}/{{AACVPR}}/{{AAPA}}/{{ABC}}/{{ACPM}}/{{ADA}}/{{AGS}}/{{APhA}}/{{ASPC}}/{{NLA}}/{{PCNA Guideline}} on the {{Management}} of {{Blood Cholesterol}}},
  author = {{Grundy Scott M.} and {Stone Neil J.} and {Bailey Alison L.} and {Beam Craig} and {Birtcher Kim K.} and {Blumenthal Roger S.} and {Braun Lynne T.} and {de Ferranti Sarah} and {Faiella-Tommasino Joseph} and {Forman Daniel E.} and {Goldberg Ronald} and {Heidenreich Paul A.} and {Hlatky Mark A.} and {Jones Daniel W.} and {Lloyd-Jones Donald} and {Lopez-Pajares Nuria} and {Ndumele Chiadi E.} and {Orringer Carl E.} and {Peralta Carmen A.} and {Saseen Joseph J.} and {Smith Sidney C.} and {Sperling Laurence} and {Virani Salim S.} and {Yeboah Joseph}},
  date = {2019-06-18},
  journaltitle = {Circulation},
  volume = {139},
  number = {25},
  pages = {e1082-e1143},
  publisher = {{American Heart Association}},
  doi = {10.1161/CIR.0000000000000625},
  url = {https://www.ahajournals.org/doi/10.1161/CIR.0000000000000625},
  urldate = {2021-05-06},
  file = {/Users/christopherboyer/Zotero/storage/3R2ECT5I/Grundy Scott M. et al. - 2019 - 2018 AHAACCAACVPRAAPAABCACPMADAAGSAPhAASP.pdf;/Users/christopherboyer/Zotero/storage/KUK7GXDL/CIR.html}
}

@article{harrell_multivariable_1996,
  title = {Multivariable {{Prognostic Models}}: {{Issues}} in {{Developing Models}}, {{Evaluating Assumptions}} and {{Adequacy}}, and {{Measuring}} and {{Reducing Errors}}},
  shorttitle = {Multivariable {{Prognostic Models}}},
  author = {Harrell, Frank E. and Lee, Kerry L. and Mark, Daniel B.},
  date = {1996},
  journaltitle = {Statistics in Medicine},
  volume = {15},
  number = {4},
  pages = {361--387},
  issn = {1097-0258},
  doi = {10.1002/(SICI)1097-0258(19960229)15:4<361::AID-SIM168>3.0.CO;2-4},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291097-0258%2819960229%2915%3A4%3C361%3A%3AAID-SIM168%3E3.0.CO%3B2-4},
  urldate = {2020-07-16},
  abstract = {Multivariable regression models are powerful tools that are used frequently in studies of clinical outcomes. These models can use a mixture of categorical and continuous variables and can handle partially observed (censored) responses. However, uncritical application of modelling techniques can result in models that poorly fit the dataset at hand, or, even more likely, inaccurately predict outcomes on new subjects. One must know how to measure qualities of a model's fit in order to avoid poorly fitted or overfitted models. Measurement of predictive accuracy can be difficult for survival time data in the presence of censoring. We discuss an easily interpretable index of predictive discrimination as well as methods for assessing calibration of predicted survival probabilities. Both types of predictive accuracy should be unbiasedly validated using bootstrapping or cross-validation, before using predictions in a new data series. We discuss some of the hazards of poorly fitted and overfitted regression models and present one modelling strategy that avoids many of the problems discussed. The methods described are applicable to all regression models, but are particularly needed for binary, ordinal, and time-to-event outcomes. Methods are illustrated with a survival analysis in prostate cancer using Cox regression.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/TS5KG7PV/Harrell et al_1996_Multivariable Prognostic Models.pdf;/Users/christopherboyer/Zotero/storage/5BELPKYX/(SICI)1097-0258(19960229)154361AID-SIM1683.0.html}
}

@book{hastie_elements_2009,
  title = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction},
  author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H and Friedman, Jerome H},
  date = {2009},
  volume = {2},
  publisher = {{Springer}},
  file = {/Users/christopherboyer/Zotero/storage/AEVVIZIG/Hastie et al. - 2009 - The elements of statistical learning data mining,.pdf}
}

@book{hernan_causal_2020,
  title = {Causal {{Inference}}: {{What If}}},
  author = {Hernán, Miguel A and Robins, James M},
  date = {2020},
  publisher = {{Chapman \& Hall/CRC}},
  location = {{Boca Raton}},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/SV2T525Y/Hernán and Robins - Causal Inference What If.pdf}
}

@article{hernan_instruments_2006,
  title = {Instruments for {{Causal Inference}}},
  author = {Hernan, Miguel A and Robins, James M},
  date = {2006},
  volume = {17},
  number = {4},
  abstract = {The use of instrumental variable (IV) methods is attractive because, even in the presence of unmeasured confounding, such methods may consistently estimate the average causal effect of an exposure on an outcome. However, for this consistent estimation to be achieved, several strong conditions must hold. We review the definition of an instrumental variable, describe the conditions required to obtain consistent estimates of causal effects, and explore their implications in the context of a recent application of the instrumental variables approach. We also present (1) a description of the connection between 4 causal models— counterfactuals, causal directed acyclic graphs, nonparametric structural equation models, and linear structural equation models—that have been used to describe instrumental variables methods; (2) a unified presentation of IV methods for the average causal effect in the study population through structural mean models; and (3) a discussion and new extensions of instrumental variables methods based on assumptions of monotonicity.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/GAM4I3EW/Hernan and Robins - 2006 - Instruments for Causal Inference.pdf}
}

@article{hernan_second_2019,
  title = {A {{Second Chance}} to {{Get Causal Inference Right}}: {{A Classification}} of {{Data Science Tasks}}},
  shorttitle = {A {{Second Chance}} to {{Get Causal Inference Right}}},
  author = {Hernán, Miguel A. and Hsu, John and Healy, Brian},
  date = {2019-01-02},
  journaltitle = {CHANCE},
  volume = {32},
  number = {1},
  pages = {42--49},
  issn = {0933-2480, 1867-2280},
  doi = {10.1080/09332480.2019.1579578},
  url = {https://www.tandfonline.com/doi/full/10.1080/09332480.2019.1579578},
  urldate = {2020-08-17},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/ZJ3R52LQ/Hernán et al. - 2019 - A Second Chance to Get Causal Inference Right A C.pdf}
}

@article{hoogland_tutorial_2021,
  title = {A Tutorial on Individualized Treatment Effect Prediction from Randomized Trials with a Binary Endpoint},
  author = {Hoogland, Jeroen and IntHout, Joanna and Belias, Michail and Rovers, Maroeska M. and Riley, Richard D. and E. Harrell Jr, Frank and Moons, Karel G. M. and Debray, Thomas P. A. and Reitsma, Johannes B.},
  date = {2021},
  journaltitle = {Statistics in Medicine},
  volume = {40},
  number = {26},
  pages = {5961--5981},
  issn = {1097-0258},
  doi = {10.1002/sim.9154},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.9154},
  urldate = {2023-06-19},
  abstract = {Randomized trials typically estimate average relative treatment effects, but decisions on the benefit of a treatment are possibly better informed by more individualized predictions of the absolute treatment effect. In case of a binary outcome, these predictions of absolute individualized treatment effect require knowledge of the individual's risk without treatment and incorporation of a possibly differential treatment effect (ie, varying with patient characteristics). In this article, we lay out the causal structure of individualized treatment effect in terms of potential outcomes and describe the required assumptions that underlie a causal interpretation of its prediction. Subsequently, we describe regression models and model estimation techniques that can be used to move from average to more individualized treatment effect predictions. We focus mainly on logistic regression-based methods that are both well-known and naturally provide the required probabilistic estimates. We incorporate key components from both causal inference and prediction research to arrive at individualized treatment effect predictions. While the separate components are well known, their successful amalgamation is very much an ongoing field of research. We cut the problem down to its essentials in the setting of a randomized trial, discuss the importance of a clear definition of the estimand of interest, provide insight into the required assumptions, and give guidance with respect to modeling and estimation options. Simulated data illustrate the potential of different modeling options across scenarios that vary both average treatment effect and treatment effect heterogeneity. Two applied examples illustrate individualized treatment effect prediction in randomized trial data.},
  langid = {english},
  keywords = {causal inference,personalized medicine,prediction,regression,treatment effect},
  file = {/Users/christopherboyer/Zotero/storage/XPRRMX9U/Hoogland et al. - 2021 - A tutorial on individualized treatment effect pred.pdf;/Users/christopherboyer/Zotero/storage/YD9HXW3M/sim.html}
}

@online{kennedy_towards_2022,
  title = {Towards Optimal Doubly Robust Estimation of Heterogeneous Causal Effects},
  author = {Kennedy, Edward H.},
  date = {2022-05-26},
  eprint = {2004.14497},
  eprinttype = {arxiv},
  eprintclass = {math, stat},
  url = {http://arxiv.org/abs/2004.14497},
  urldate = {2022-10-31},
  abstract = {Heterogeneous effect estimation plays a crucial role in causal inference, with applications across medicine and social science. Many methods for estimating conditional average treatment effects (CATEs) have been proposed in recent years, but there are important theoretical gaps in understanding if and when such methods are optimal. This is especially true when the CATE has nontrivial structure (e.g., smoothness or sparsity). Our work contributes in several main ways. First, we study a two-stage doubly robust CATE estimator and give a generic model-free error bound, which, despite its generality, yields sharper results than those in the current literature. We apply the bound to derive error rates in nonparametric models with smoothness or sparsity, and give sufficient conditions for oracle efficiency. Underlying our error bound is a general oracle inequality for regression with estimated or imputed outcomes, which is of independent interest; this is the second main contribution. The third contribution is aimed at understanding the fundamental statistical limits of CATE estimation. To that end, we propose and study a local polynomial adaptation of double-residual regression. We show that this estimator can be oracle efficient under even weaker conditions, if used with a specialized form of sample splitting and careful choices of tuning parameters. These are the weakest conditions currently found in the literature, and we conjecture that they are minimal in a minimax sense. We go on to give error bounds in the non-trivial regime where oracle rates cannot be achieved. Some finite-sample properties are explored with simulations.},
  pubstate = {preprint},
  keywords = {Mathematics - Statistics Theory},
  file = {/Users/christopherboyer/Zotero/storage/4YY3IN7N/Kennedy - 2022 - Towards optimal doubly robust estimation of hetero.pdf;/Users/christopherboyer/Zotero/storage/JWJPPYJM/2004.html}
}

@article{kent_personalized_2018,
  title = {Personalized Evidence Based Medicine: Predictive Approaches to Heterogeneous Treatment Effects},
  shorttitle = {Personalized Evidence Based Medicine},
  author = {Kent, David M. and Steyerberg, Ewout and van Klaveren, David},
  date = {2018-12-10},
  journaltitle = {BMJ},
  volume = {363},
  eprint = {30530757},
  eprinttype = {pmid},
  pages = {k4245},
  publisher = {{British Medical Journal Publishing Group}},
  issn = {0959-8138, 1756-1833},
  doi = {10.1136/bmj.k4245},
  url = {https://www.bmj.com/content/363/bmj.k4245},
  urldate = {2023-06-19},
  abstract = {The use of evidence from clinical trials to support decisions for individual patients is a form of “reference class forecasting”: implicit predictions for an individual are made on the basis of outcomes in a reference class of “similar” patients treated with alternative therapies. Evidence based medicine has generally emphasized the broad reference class of patients qualifying for a trial. Yet patients in a trial (and in clinical practice) differ from one another in many ways that can affect the outcome of interest and the potential for benefit. The central goal of personalized medicine, in its various forms, is to narrow the reference class to yield more patient specific effect estimates to support more individualized clinical decision making. This article will review fundamental conceptual problems with the prediction of outcome risk and heterogeneity of treatment effect (HTE), as well as the limitations of conventional (one-variable-at-a-time) subgroup analysis. It will also discuss several regression based approaches to “predictive” heterogeneity of treatment effect analysis, including analyses based on “risk modeling” (such as stratifying trial populations by their risk of the primary outcome or their risk of serious treatment-related harms) and analysis based on “effect modeling” (which incorporates modifiers of relative effect). It will illustrate these approaches with clinical examples and discuss their respective strengths and vulnerabilities.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/2BLLKNU6/Kent et al. - 2018 - Personalized evidence based medicine predictive a.pdf}
}

@article{kent_predictive_2020,
  title = {The {{Predictive Approaches}} to {{Treatment}} Effect {{Heterogeneity}} ({{PATH}}) {{Statement}}},
  author = {Kent, David M. and Paulus, Jessica K. and van Klaveren, David and D'Agostino, Ralph and Goodman, Steve and Hayward, Rodney and Ioannidis, John P.A. and Patrick-Lake, Bray and Morton, Sally and Pencina, Michael and Raman, Gowri and Ross, Joseph S. and Selker, Harry P. and Varadhan, Ravi and Vickers, Andrew and Wong, John B. and Steyerberg, Ewout W.},
  options = {useprefix=true},
  date = {2020-01-07},
  journaltitle = {Ann Intern Med},
  volume = {172},
  number = {1},
  pages = {35--45},
  publisher = {{American College of Physicians}},
  issn = {0003-4819},
  doi = {10.7326/M18-3667},
  url = {https://www.acpjournals.org/doi/10.7326/M18-3667},
  urldate = {2022-12-15},
  file = {/Users/christopherboyer/Zotero/storage/98NN8QA4/Kent et al. - 2020 - The Predictive Approaches to Treatment effect Hete.pdf}
}

@article{li_estimating_2022,
  title = {Estimating the Area under the {{ROC}} Curve When Transporting a Prediction Model to a Target Population},
  author = {Li, Bing and Gatsonis, Constantine and Dahabreh, Issa J. and Steingrimsson, Jon A.},
  date = {2022-11-25},
  journaltitle = {Biometrics},
  pages = {biom.13796},
  issn = {0006-341X, 1541-0420},
  doi = {10.1111/biom.13796},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/biom.13796},
  urldate = {2022-12-15},
  abstract = {We propose methods for estimating the area under the receiver operating characteristic (ROC) curve (AUC) of a prediction model in a target population that differs from the source population that provided the data used for original model development. If covariates that are associated with model performance, as measured by the AUC, have a different distribution in the source and target populations, then AUC estimators that only use data from the source population will not reflect model performance in the target population. Here, we provide identification results for the AUC in the target population when outcome and covariate data are available from the sample of the source population, but only covariate data are available from the sample of the target population. In this setting, we propose three estimators for the AUC in the target population and show that they are consistent and asymptotically normal. We evaluate the finite-sample performance of the estimators using simulations and use them to estimate the AUC in a nationally representative target population from the National Health and Nutrition Examination Survey for a lung cancer risk prediction model developed using source population data from the National Lung Screening Trial.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/GDD4YP4N/Li et al. - 2022 - Estimating the area under the ROC curve when trans.pdf}
}

@article{lin_scoping_2021,
  title = {A Scoping Review of Causal Methods Enabling Predictions under Hypothetical Interventions},
  author = {Lin, Lijing and Sperrin, Matthew and Jenkins, David A. and Martin, Glen P. and Peek, Niels},
  date = {2021-02-04},
  journaltitle = {Diagn Progn Res},
  volume = {5},
  number = {1},
  pages = {3},
  issn = {2397-7523},
  doi = {10.1186/s41512-021-00092-9},
  url = {https://doi.org/10.1186/s41512-021-00092-9},
  urldate = {2022-12-15},
  abstract = {The methods with which prediction models are usually developed mean that neither the parameters nor the predictions should be interpreted causally. For many applications, this is perfectly acceptable. However, when prediction models are used to support decision making, there is often a need for predicting outcomes under hypothetical interventions.},
  langid = {english},
  keywords = {Causal inference,Clinical prediction models,Counterfactual prediction,Statistical modeling},
  file = {/Users/christopherboyer/Zotero/storage/TNA89ZL3/Lin et al. - 2021 - A scoping review of causal methods enabling predic.pdf}
}

@article{mehrotra_transporting_2021,
  title = {Transporting {{Subgroup Analyses}} of {{Randomized Controlled Trials}} for {{Planning Implementation}} of {{New Interventions}}},
  author = {Mehrotra, Megha L and Westreich, Daniel and Glymour, M Maria and Geng, Elvin and Glidden, David V},
  date = {2021-08-01},
  journaltitle = {American Journal of Epidemiology},
  volume = {190},
  number = {8},
  pages = {1671--1680},
  issn = {0002-9262},
  doi = {10.1093/aje/kwab045},
  url = {https://doi.org/10.1093/aje/kwab045},
  urldate = {2023-06-17},
  abstract = {Subgroup analyses of randomized controlled trials guide resource allocation and implementation of new interventions by identifying groups of individuals who are likely to benefit most from the intervention. Unfortunately, trial populations are rarely representative of the target populations of public health or clinical interest. Unless the relevant differences between trial and target populations are accounted for, subgroup results from trials might not reflect which groups in the target population will benefit most from the intervention. Transportability provides a rigorous framework for applying results derived in potentially highly selected study populations to external target populations. The method requires that researchers measure and adjust for all variables that 1) modify the effect of interest and 2) differ between the target and trial populations. To date, applications of transportability have focused on the external validity of overall study results and understanding within-trial heterogeneity; however, this approach has not yet been used for subgroup analyses of trials. Through an example from the Iniciativa Profilaxis Pre-Exposición (iPrEx) study (multiple countries, 2007–2010) of preexposure prophylaxis for human immunodeficiency virus, we illustrate how transporting subgroup analyses can produce target-specific subgroup effect estimates and numbers needed to treat. This approach could lead to more tailored and accurate guidance for resource allocation and cost-effectiveness analyses.},
  file = {/Users/christopherboyer/Zotero/storage/VUIFRJYG/Mehrotra et al. - 2021 - Transporting Subgroup Analyses of Randomized Contr.pdf;/Users/christopherboyer/Zotero/storage/P5D3IYEB/6145106.html}
}

@online{morrison_robust_2022,
  title = {Robust {{Estimation}} of {{Loss-Based Measures}} of {{Model Performance}} under {{Covariate Shift}}},
  author = {Morrison, Samantha and Gatsonis, Constantine and Dahabreh, Issa J. and Li, Bing and Steingrimsson, Jon A.},
  date = {2022-10-04},
  eprint = {2210.01980},
  eprinttype = {arxiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/2210.01980},
  urldate = {2022-12-15},
  abstract = {We present methods for estimating loss-based measures of the performance of a prediction model in a target population that differs from the source population in which the model was developed, in settings where outcome and covariate data are available from the source population but only covariate data are available on a simple random sample from the target population. Prior work adjusting for differences between the two populations has used various weighting estimators with inverse odds or density ratio weights. Here, we develop more robust estimators for the target population risk (expected loss) that can be used with data-adaptive (e.g., machine learning-based) estimation of nuisance parameters. We examine the large-sample properties of the estimators and evaluate finite sample performance in simulations. Last, we apply the methods to data from lung cancer screening using nationally representative data from the National Health and Nutrition Examination Survey (NHANES) and extend our methods to account for the complex survey design of the NHANES.},
  pubstate = {preprint},
  keywords = {Statistics - Methodology},
  file = {/Users/christopherboyer/Zotero/storage/XN43N64I/Morrison et al. - 2022 - Robust Estimation of Loss-Based Measures of Model .pdf;/Users/christopherboyer/Zotero/storage/YS6CUI3A/2210.html}
}

@article{pajouheshnia_accounting_2017,
  title = {Accounting for Treatment Use When Validating a Prognostic Model: A Simulation Study},
  shorttitle = {Accounting for Treatment Use When Validating a Prognostic Model},
  author = {Pajouheshnia, Romin and Peelen, Linda M. and Moons, Karel G. M. and Reitsma, Johannes B. and Groenwold, Rolf H. H.},
  date = {2017-07-14},
  journaltitle = {BMC Med Res Methodol},
  volume = {17},
  number = {1},
  pages = {103},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0375-8},
  url = {https://doi.org/10.1186/s12874-017-0375-8},
  urldate = {2020-08-17},
  abstract = {Prognostic models often show poor performance when applied to independent validation data sets. We illustrate how treatment use in a validation set can affect measures of model performance and present the uses and limitations of available analytical methods to account for this using simulated data.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/Q9Y3J57K/Pajouheshnia et al_2017_Accounting for treatment use when validating a prognostic model.pdf}
}

@article{pajouheshnia_accounting_2017-1,
  title = {Accounting for Treatment Use When Validating a Prognostic Model: A Simulation Study},
  shorttitle = {Accounting for Treatment Use When Validating a Prognostic Model},
  author = {Pajouheshnia, Romin and Peelen, Linda M. and Moons, Karel G. M. and Reitsma, Johannes B. and Groenwold, Rolf H. H.},
  date = {2017-07-14},
  journaltitle = {BMC Medical Research Methodology},
  volume = {17},
  number = {1},
  pages = {103},
  issn = {1471-2288},
  doi = {10.1186/s12874-017-0375-8},
  url = {https://doi.org/10.1186/s12874-017-0375-8},
  urldate = {2022-12-15},
  abstract = {Prognostic models often show poor performance when applied to independent validation data sets. We illustrate how treatment use in a validation set can affect measures of model performance and present the uses and limitations of available analytical methods to account for this using simulated data.},
  keywords = {Inverse Probability Weighting (IPW),Model Discrimination,Prognostic Model,Untreated Risk,Validation Data Set},
  file = {/Users/christopherboyer/Zotero/storage/TQ8I2A6I/Pajouheshnia et al. - 2017 - Accounting for treatment use when validating a pro.pdf;/Users/christopherboyer/Zotero/storage/LWVQVI6A/s12874-017-0375-8.html}
}

@article{piccininni_directed_2020-1,
  title = {Directed Acyclic Graphs and Causal Thinking in Clinical Risk Prediction Modeling},
  author = {Piccininni, Marco and Konigorski, Stefan and Rohmann, Jessica L. and Kurth, Tobias},
  date = {2020-07-02},
  journaltitle = {BMC Medical Research Methodology},
  volume = {20},
  number = {1},
  pages = {179},
  issn = {1471-2288},
  doi = {10.1186/s12874-020-01058-z},
  url = {https://doi.org/10.1186/s12874-020-01058-z},
  urldate = {2023-05-17},
  abstract = {In epidemiology, causal inference and prediction modeling methodologies have been historically distinct. Directed Acyclic Graphs (DAGs) are used to model a priori causal assumptions and inform variable selection strategies for causal questions. Although tools originally designed for prediction are finding applications in causal inference, the counterpart has remained largely unexplored. The aim of this theoretical and simulation-based study is to assess the potential benefit of using DAGs in clinical risk prediction modeling.},
  keywords = {Causality,Clinical risk prediction,Directed acyclic graph,Markov blanket,Prediction models,Predictor selection,Transportability},
  file = {/Users/christopherboyer/Zotero/storage/S6TZ3845/Piccininni et al. - 2020 - Directed acyclic graphs and causal thinking in cli.pdf;/Users/christopherboyer/Zotero/storage/IN2H3SV2/s12874-020-01058-z.html}
}

@online{robertson_estimating_2021,
  title = {Estimating Subgroup Effects in Generalizability and Transportability Analyses},
  author = {Robertson, Sarah E. and Steingrimsson, Jon A. and Joyce, Nina R. and Stuart, Elizabeth A. and Dahabreh, Issa J.},
  date = {2021-09-28},
  eprint = {2109.14075},
  eprinttype = {arxiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/2109.14075},
  urldate = {2023-06-17},
  abstract = {Methods for extending -- generalizing or transporting -- inferences from a randomized trial to a target population involve conditioning on a large set of covariates that is sufficient for rendering the randomized and non-randomized groups exchangeable. Yet, decision-makers are often interested in examining treatment effects in subgroups of the target population defined in terms of only a few discrete covariates. Here, we propose methods for estimating subgroup-specific potential outcome means and average treatment effects in generalizability and transportability analyses, using outcome model-based (g-formula), weighting, and augmented weighting estimators. We consider estimating subgroup-specific average treatment effects in the target population and its non-randomized subset, and provide methods that are appropriate both for nested and non-nested trial designs. As an illustration, we apply the methods to data from the Coronary Artery Surgery Study to compare the effect of surgery plus medical therapy versus medical therapy alone for chronic coronary artery disease in subgroups defined by history of myocardial infarction.},
  pubstate = {preprint},
  keywords = {Statistics - Methodology},
  file = {/Users/christopherboyer/Zotero/storage/3GUXSAW8/Robertson et al. - 2021 - Estimating subgroup effects in generalizability an.pdf;/Users/christopherboyer/Zotero/storage/UP6CJWEX/2109.html}
}

@article{robertson_regression-based_2023,
  title = {Regression-Based Estimation of Heterogeneous Treatment Effects When Extending Inferences from a Randomized Trial to a Target Population},
  author = {Robertson, Sarah E. and Steingrimsson, Jon A. and Dahabreh, Issa J.},
  date = {2023-02-01},
  journaltitle = {European Journal of Epidemiology},
  volume = {38},
  number = {2},
  pages = {123--133},
  issn = {1573-7284},
  doi = {10.1007/s10654-022-00901-5},
  url = {https://doi.org/10.1007/s10654-022-00901-5},
  abstract = {Most work on extending (generalizing or transporting) inferences from a randomized trial to a target population has focused on estimating average treatment effects (i.e., averaged over the target population’s covariate distribution). Yet, in the presence of strong effect modification by baseline covariates, the average treatment effect in the target population may be less relevant for guiding treatment decisions. Instead, the conditional average treatment effect (CATE) as a function of key effect modifiers may be a more useful estimand. Recent work on estimating target population CATEs using baseline covariate, treatment, and outcome data from the trial and covariate data from the target population only allows for the examination of heterogeneity over distinct subgroups. We describe flexible pseudo-outcome regression modeling methods for estimating target population CATEs conditional on discrete or continuous baseline covariates when the trial is embedded in a sample from the target population (i.e., in nested trial designs). We construct pointwise confidence intervals for the CATE at a specific value of the effect modifiers and uniform confidence bands for the CATE function. Last, we illustrate the methods using data from the Coronary Artery Surgery Study (CASS) to estimate CATEs given history of myocardial infarction and baseline ejection fraction value in the target population of all trial-eligible patients with stable ischemic heart disease.},
  file = {/Users/christopherboyer/Zotero/storage/TD4R49WS/Robertson et al. - 2023 - Regression-based estimation of heterogeneous treat.pdf}
}

@article{robins_estimating_1992,
  title = {Estimating {{Exposure Effects}} by {{Modelling}} the {{Expectation}} of {{Exposure Conditional}} on {{Confounders}}},
  author = {Robins, James M. and Mark, Steven D. and Newey, Whitney K.},
  date = {1992},
  journaltitle = {Biometrics},
  volume = {48},
  number = {2},
  eprint = {2532304},
  eprinttype = {jstor},
  pages = {479--495},
  publisher = {{[Wiley, International Biometric Society]}},
  issn = {0006-341X},
  doi = {10.2307/2532304},
  url = {https://www.jstor.org/stable/2532304},
  urldate = {2023-08-23},
  abstract = {In order to estimate the causal effects of one or more exposures or treatments on an outcome of interest, one has to account for the effect of "confounding factors" which both covary with the exposures or treatments and are independent predictors of the outcome. In this paper we present regression methods which, in contrast to standard methods, adjust for the confounding effect of multiple continuous or discrete covariates by modelling the conditional expectation of the exposures or treatments given the confounders. In the special case of a univariate dichotomous exposure or treatment, this conditional expectation is identical to what Rosenbaum and Rubin have called the propensity score. They have also proposed methods to estimate causal effects by modelling the propensity score. Our methods generalize those of Rosenbaum and Rubin in several ways. First, our approach straightforwardly allows for multivariate exposures or treatments, each of which may be continuous, ordinal, or discrete. Second, even in the case of a single dichotomous exposure, our approach does not require subclassification or matching on the propensity score so that the potential for "residual confounding," i.e., bias, due to incomplete matching is avoided. Third, our approach allows a rather general formalization of the idea that it is better to use the "estimated propensity score" than the true propensity score even when the true score is known. The additional power of our approach derives from the fact that we assume the causal effects of the exposures or treatments can be described by the parametric component of a semiparametric regression model. To illustrate our methods, we reanalyze the effect of current cigarette smoking on the level of forced expiratory volume in one second in a cohort of 2,713 adult white males. We compare the results with those obtained using standard methods.},
  file = {/Users/christopherboyer/Zotero/storage/3ZGPSX2R/Robins et al. - 1992 - Estimating Exposure Effects by Modelling the Expec.pdf}
}

@article{robins_estimation_1994,
  title = {Estimation of {{Regression Coefficients When Some Regressors Are Not Always Observed}}},
  author = {Robins, James M. and Rotnitzky, Andrea and Zhao, Lue Ping},
  date = {1994},
  journaltitle = {Journal of the American Statistical Association},
  volume = {89},
  number = {427},
  eprint = {2290910},
  eprinttype = {jstor},
  pages = {846--866},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2290910},
  url = {http://www.jstor.org/stable/2290910},
  urldate = {2020-11-12},
  abstract = {In applied problems it is common to specify a model for the conditional mean of a response given a set of regressors. A subset of the regressors may be missing for some study subjects either by design or happenstance. In this article we propose a new class of semiparametric estimators, based on inverse probability weighted estimating equations, that are consistent for parameter vector α\textsubscript{0} of the conditional mean model when the data are missing at random in the sense of Rubin and the missingness probabilities are either known or can be parametrically modeled. We show that the asymptotic variance of the optimal estimator in our class attains the semiparametric variance bound for the model by first showing that our estimation problem is a special case of the general problem of parameter estimation in an arbitrary semiparametric model in which the data are missing at random and the probability of observing complete data is bounded away from 0, and then deriving a representation for the efficient score, the semiparametric variance bound, and the influence function of any regular, asymptotically linear estimator in this more general estimation problem. Because the optimal estimator depends on the unknown probability law generating the data, we propose locally and globally adaptive semiparametric efficient estimators. We compare estimators in our class with previously proposed estimators. We show that each previous estimator is asymptotically equivalent to some, usually inefficient, estimator in our class. This equivalence is a consequence of a proposition stating that every regular asymptotic linear estimator of α\textsubscript{0} is asymptotically equivalent to some estimator in our class. We compare various estimators in a small simulation study and offer some practical recommendations.},
  file = {/Users/christopherboyer/Zotero/storage/3XRXNXIK/Robins et al_1994_Estimation of Regression Coefficients When Some Regressors Are Not Always.pdf}
}

@article{robins_graphical_1987,
  title = {A Graphical Approach to the Identification and Estimation of Causal Parameters in Mortality Studies with Sustained Exposure Periods},
  author = {Robins, James},
  date = {1987-01},
  journaltitle = {Journal of Chronic Diseases},
  volume = {40},
  pages = {139S-161S},
  issn = {00219681},
  doi = {10.1016/S0021-9681(87)80018-8},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0021968187800188},
  urldate = {2022-12-15},
  abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, independent risk factors for death commonly determine subsequent exposure to the study agent. For example, in occupational mortality studies, date of termination of employment is both a determinant of subsequent exposure to the chemical agent under study (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When a risk factor determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure can underestimate the true effect of exposure on mortality, whether or not one adjusts for the risk factor in the analysis. This observation raises the question, "Which, if any, empirical population parameter can be causally interpreted as the true effect of exposure in observational mortality studies?" In answer, we offer a graphical approach to the identification and estimation of causal parameters in mortality studies with sustained exposure periods. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers using our approach and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all cause and lung cancer mortality, which standard methods failed to detect. The analytic approach introduced in this paper may be necessary to control bias in any epidemiologic study in which there exists a risk factor which both determines subsequent exposure and is determined by previous exposure to the agent under study.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/53G3EYXD/Robins - 1987 - A graphical approach to the identification and est.pdf}
}

@article{robins_higher_2008,
  title = {Higher Order Influence Functions and Minimax Estimation of Nonlinear Functionals},
  author = {Robins, James and Li, Lingling and Tchetgen, Eric and van der Vaart, Aad},
  options = {useprefix=true},
  date = {2008-01-01},
  journaltitle = {Probability and Statistics: Essays in Honor of David A. Freedman},
  volume = {2},
  pages = {335--422},
  publisher = {{Institute of Mathematical Statistics}},
  doi = {10.1214/193940307000000527},
  url = {https://projecteuclid.org/ebooks/institute-of-mathematical-statistics-collections/Probability-and-Statistics--Essays-in-Honor-of-David-A/chapter/Higher-order-influence-functions-and-minimax-estimation-of-nonlinear-functionals/10.1214/193940307000000527},
  urldate = {2022-12-15},
  abstract = {{$<$}!-- *** Custom HTML *** --{$><$}p{$>$} We present a theory of point and interval estimation for nonlinear functionals in parametric, semi-, and non-parametric models based on higher order influence functions (Robins (2004), Section 9; Li et al. (2004), Tchetgen et al. (2006), Robins et al. (2007)). Higher order influence functions are higher order U-statistics. Our theory extends the first order semiparametric theory of Bickel et al. (1993) and van der Vaart (1991) by incorporating the theory of higher order scores considered by Pfanzagl (1990), Small and McLeish (1994) and Lindsay and Waterman (1996). The theory reproduces many previous results, produces new non-\$\textbackslash sqrt\{n\}\$ results, and opens up the ability to perform optimal non-\$\textbackslash sqrt\{n\}\$ inference in complex high dimensional models. We present novel rate-optimal point and interval estimators for various functionals of central importance to biostatistics in settings in which estimation at the expected \$\textbackslash sqrt\{n\}\$ rate is not possible, owing to the curse of dimensionality. We also show that our higher order influence functions have a multi-robustness property that extends the double robustness property of first order influence functions described by Robins and Rotnitzky (2001) and van der Laan and Robins (2003). {$<$}/p{$>$}},
  file = {/Users/christopherboyer/Zotero/storage/FFPC4U34/Robins et al. - 2008 - Higher order influence functions and minimax estim.pdf}
}

@article{robins_new_1986,
  title = {A New Approach to Causal Inference in Mortality Studies with a Sustained Exposure Period—Application to Control of the Healthy Worker Survivor Effect},
  author = {Robins, James},
  date = {1986-01-01},
  journaltitle = {Mathematical Modelling},
  volume = {7},
  number = {9},
  pages = {1393--1512},
  issn = {0270-0255},
  doi = {10.1016/0270-0255(86)90088-6},
  url = {http://www.sciencedirect.com/science/article/pii/0270025586900886},
  urldate = {2020-07-09},
  abstract = {In observational cohort mortality studies with prolonged periods of exposure to the agent under study, it is not uncommon for risk factors for death to be determinants of subsequent exposure. For instance, in occupational mortality studies date of termination of employment is both a determinant of future exposure (since terminated individuals receive no further exposure) and an independent risk factor for death (since disabled individuals tend to leave employment). When current risk factor status determines subsequent exposure and is determined by previous exposure, standard analyses that estimate age-specific mortality rates as a function of cumulative exposure may underestimate the true effect of exposure on mortality whether or not one adjusts for the risk factor in the analysis. This observation raises the question, which if any population parameters can be given a causal interpretation in observational mortality studies? In answer, we offer a graphical approach to the identification and computation of causal parameters in mortality studies with sustained exposure periods. This approach is shown to be equivalent to an approach in which the observational study is identified with a hypothetical double-blind randomized trial in which data on each subject's assigned treatment protocol has been erased from the data file. Causal inferences can then be made by comparing mortality as a function of treatment protocol, since, in a double-blind randomized trial missing data on treatment protocol, the association of mortality with treatment protocol can still be estimated. We reanalyze the mortality experience of a cohort of arsenic-exposed copper smelter workers with our method and compare our results with those obtained using standard methods. We find an adverse effect of arsenic exposure on all-cause and lung cancer mortality which standard methods fail to detect.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/CLIME2RP/Robins_1986_A new approach to causal inference in mortality studies with a sustained.pdf;/Users/christopherboyer/Zotero/storage/Q5ZA6LD4/0270025586900886.html}
}

@inproceedings{robins_sensitivity_2000,
  title = {Sensitivity {{Analysis}} for {{Selection}} Bias and Unmeasured {{Confounding}} in Missing {{Data}} and {{Causal}} Inference Models},
  booktitle = {Statistical {{Models}} in {{Epidemiology}}, the {{Environment}}, and {{Clinical Trials}}},
  author = {Robins, James M. and Rotnitzky, Andrea and Scharfstein, Daniel O.},
  editor = {Halloran, M. Elizabeth and Berry, Donald},
  date = {2000},
  series = {The {{IMA Volumes}} in {{Mathematics}} and Its {{Applications}}},
  pages = {1--94},
  publisher = {{Springer}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4612-1284-3_1},
  abstract = {In both observational and randomized studies, subjects commonly drop out of the study (i.e., become censored) before end of follow-up. If, conditional on the history of the observed data up to t, the hazard of dropping out of the study (i.e., censoring) at time t does not depend on the possibly unobserved data subsequent to t, we say drop-out is ignorable or explainable (Rubin, 1976). On the other hand, if the hazard of drop-out depends on the possibly unobserved future, we say drop-out is non-ignorable or, equivalently, that there is selection bias on unobservables. Neither the existence of selection bias on unobservables nor its magnitude is identifiable from the joint distribution of the observables. In view of this fact, we argue that the data analyst should conduct a “sensitivity analysis” to quantify how one’s inference concerning an outcome of interest varies as a function of the magnitude of non-identifiable selection bias.},
  isbn = {978-1-4612-1284-3},
  langid = {english},
  keywords = {Consistency Assumption,Current Status Data,Influence Function,Marginal Structural Model,Semiparametric Model},
  file = {/Users/christopherboyer/Zotero/storage/HUG7Z4C2/Robins et al. - 2000 - Sensitivity Analysis for Selection bias and unmeas.pdf}
}

@article{rolling2014model,
  title = {Model Selection for Estimating Treatment Effects},
  author = {Rolling, Craig A and Yang, Yuhong},
  date = {2014},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {76},
  number = {4},
  pages = {749--769},
  publisher = {{Wiley Online Library}},
  file = {/Users/christopherboyer/Zotero/storage/L2TDHEBS/Rolling and Yang - 2014 - Model selection for estimating treatment effects.pdf}
}

@article{rubin_randomization_1980,
  title = {Randomization {{Analysis}} of {{Experimental Data}}: {{The Fisher Randomization Test Comment}}},
  shorttitle = {Randomization {{Analysis}} of {{Experimental Data}}},
  author = {Rubin, Donald B.},
  date = {1980},
  journaltitle = {Journal of the American Statistical Association},
  volume = {75},
  number = {371},
  eprint = {2287653},
  eprinttype = {jstor},
  pages = {591--593},
  publisher = {{[American Statistical Association, Taylor \& Francis, Ltd.]}},
  issn = {0162-1459},
  doi = {10.2307/2287653},
  url = {https://www.jstor.org/stable/2287653},
  urldate = {2023-06-15}
}

@inproceedings{schulam_reliable_2017-1,
  title = {Reliable {{Decision Support}} Using {{Counterfactual Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schulam, Peter and Saria, Suchi},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2017/hash/299a23a2291e2126b91d54f3601ec162-Abstract.html},
  urldate = {2022-12-15},
  abstract = {Decision-makers are faced with the challenge of estimating what is likely to happen when they take an action. For instance, if I choose not to treat this patient, are they likely to die? Practitioners commonly use supervised learning algorithms to fit predictive models that help decision-makers reason about likely future outcomes, but we show that this approach is unreliable, and sometimes even dangerous. The key issue is that supervised learning algorithms are highly sensitive to the policy used to choose actions in the training data, which causes the model to capture relationships that do not generalize. We propose using a different learning objective that predicts counterfactuals instead of predicting outcomes under an existing action policy as in supervised learning. To support decision-making in temporal settings, we introduce the Counterfactual Gaussian Process (CGP) to predict the counterfactual future progression of continuous-time trajectories under sequences of future actions. We demonstrate the benefits of the CGP on two important decision-support tasks: risk prediction and “what if?” reasoning for individualized treatment planning.},
  file = {/Users/christopherboyer/Zotero/storage/JWXI7RUF/Schulam and Saria - 2017 - Reliable Decision Support using Counterfactual Mod.pdf}
}

@online{schuler_comparison_2018,
  title = {A Comparison of Methods for Model Selection When Estimating Individual Treatment Effects},
  author = {Schuler, Alejandro and Baiocchi, Michael and Tibshirani, Robert and Shah, Nigam},
  date = {2018-06-13},
  eprint = {1804.05146},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1804.05146},
  urldate = {2022-12-15},
  abstract = {Practitioners in medicine, business, political science, and other fields are increasingly aware that decisions should be personalized to each patient, customer, or voter. A given treatment (e.g. a drug or advertisement) should be administered only to those who will respond most positively, and certainly not to those who will be harmed by it. Individual-level treatment effects can be estimated with tools adapted from machine learning, but different models can yield contradictory estimates. Unlike risk prediction models, however, treatment effect models cannot be easily evaluated against each other using a held-out test set because the true treatment effect itself is never directly observed. Besides outcome prediction accuracy, several metrics that can leverage held-out data to evaluate treatment effects models have been proposed, but they are not widely used. We provide a didactic framework that elucidates the relationships between the different approaches and compare them all using a variety of simulations of both randomized and observational data. Our results show that researchers estimating heterogenous treatment effects need not limit themselves to a single model-fitting algorithm. Instead of relying on a single method, multiple models fit by a diverse set of algorithms should be evaluated against each other using an objective function learned from the validation set. The model minimizing that objective should be used for estimating the individual treatment effect for future individuals.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/christopherboyer/Zotero/storage/VTWMTN5E/Schuler et al. - 2018 - A comparison of methods for model selection when e.pdf;/Users/christopherboyer/Zotero/storage/U7DKNPLU/1804.html}
}

@article{seamans_generalizability_2021,
  title = {Generalizability of {{Subgroup Effects}}},
  author = {Seamans, Marissa J. and Hong, Hwanhee and Ackerman, Benjamin and Schmid, Ian and Stuart, Elizabeth A.},
  date = {2021-05},
  journaltitle = {Epidemiology},
  volume = {32},
  number = {3},
  pages = {389--392},
  issn = {1044-3983},
  doi = {10.1097/EDE.0000000000001329},
  url = {https://journals.lww.com/10.1097/EDE.0000000000001329},
  urldate = {2023-06-17},
  abstract = {Generalizability methods are increasingly used to make inferences about the effect of interventions in target populations using a study sample. Most existing methods to generalize effects from sample to population rely on the assumption that subgroup-specific effects generalize directly. However, researchers may be concerned that in fact subgroup-specific effects differ between sample and population. In this brief report, we explore the generalizability of subgroup effects. First, we derive the bias in the sample average treatment effect estimator as an estimate of the population average treatment effect when subgroup effects in the sample do not directly generalize. Next, we present a Monte Carlo simulation to explore bias due to unmeasured heterogeneity of subgroup effects across sample and population. Finally, we examine the potential for bias in an illustrative data example. Understanding the generalizability of subgroup effects may lead to increased use of these methods for making externally valid inferences of treatment effects using a study sample.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/MNBVMDEY/Seamans et al. - 2021 - Generalizability of Subgroup Effects.pdf}
}

@article{sperrin_using_2018,
  title = {Using Marginal Structural Models to Adjust for Treatment Drop-in When Developing Clinical Prediction Models},
  author = {Sperrin, Matthew and Martin, Glen P. and Pate, Alexander and Staa, Tjeerd Van and Peek, Niels and Buchan, Iain},
  date = {2018},
  journaltitle = {Statistics in Medicine},
  volume = {37},
  number = {28},
  pages = {4142--4154},
  issn = {1097-0258},
  doi = {10.1002/sim.7913},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.7913},
  urldate = {2020-08-17},
  abstract = {Clinical prediction models (CPMs) can inform decision making about treatment initiation, which requires predicted risks assuming no treatment is given. However, this is challenging since CPMs are usually derived using data sets where patients received treatment, often initiated postbaseline as “treatment drop-ins.” This study proposes the use of marginal structural models (MSMs) to adjust for treatment drop-in. We illustrate the use of MSMs in the CPM framework through simulation studies that represent randomized controlled trials and real-world observational data and the example of statin initiation for cardiovascular disease prevention. The simulations include a binary treatment and a covariate, each recorded at two timepoints and having a prognostic effect on a binary outcome. The bias in predicted risk was examined in a model ignoring treatment, a model fitted on treatment-naïve patients (at baseline), a model including baseline treatment, and the MSM. In all simulation scenarios, all models except the MSM underestimated the risk of outcome given absence of treatment. These results were supported in the statin initiation example, which showed that ignoring statin initiation postbaseline resulted in models that significantly underestimated the risk of a cardiovascular disease event occurring within 10 years. Consequently, CPMs that do not acknowledge treatment drop-in can lead to underallocation of treatment. In conclusion, when developing CPMs to predict treatment-naïve risk, researchers should consider using MSMs to adjust for treatment drop-in, and also seek to exploit the ability of MSMs to allow estimation of individual treatment effects.},
  langid = {english},
  keywords = {clinical prediction models,counterfactual causal inference,longitudinal data,marginal structural models,treatment drop-in,validation},
  file = {/Users/christopherboyer/Zotero/storage/Z34DXC8S/Sperrin et al_2018_Using marginal structural models to adjust for treatment drop-in when.pdf;/Users/christopherboyer/Zotero/storage/EN27BKBI/sim.html}
}

@article{stefanski_calculus_2002,
  title = {The {{Calculus}} of {{M-Estimation}}},
  author = {Stefanski, Leonard A and Boos, Dennis D},
  date = {2002-02-01},
  journaltitle = {The American Statistician},
  volume = {56},
  number = {1},
  pages = {29--38},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1198/000313002753631330},
  url = {https://doi.org/10.1198/000313002753631330},
  urldate = {2022-12-15},
  abstract = {Since the seminal papers by Huber in the 1960s, M-estimation methods (also known as estimating equation methods) have been increasingly important for asymptotic analysis and approximate inference. This article illustrates the breadth and generality of the M-estimation approach, thereby facilitating its use inpractice and in the classroom as a unifying approach to the study of large-sample inference.},
  keywords = {Asymptotic variance,Central limit theorem,Estimating equations,Large-sample inference,M-estimator,Maple},
  file = {/Users/christopherboyer/Zotero/storage/GTI5PSBY/Stefanski and Boos - 2002 - The Calculus of M-Estimation.pdf}
}

@article{steingrimsson_extending_2022,
  title = {Extending Prediction Models for Use in a New Target Population with Failure Time Outcomes},
  author = {Steingrimsson, Jon A},
  date = {2022-04-07},
  journaltitle = {Biostatistics},
  pages = {kxac011},
  issn = {1465-4644, 1468-4357},
  doi = {10.1093/biostatistics/kxac011},
  url = {https://academic.oup.com/biostatistics/advance-article/doi/10.1093/biostatistics/kxac011/6564633},
  urldate = {2022-12-15},
  abstract = {Prediction models are often built and evaluated using data from a population that differs from the target population where model-derived predictions are intended to be used in. In this article, we present methods for evaluating model performance in the target population when some observations are right censored. The methods assume that outcome and covariate data are available from a source population used for model development and covariates, but no outcome data, are available from the target population. We evaluate the finite sample performance of the proposed estimators using simulations and apply the methods to transport a prediction model built using data from a lung cancer screening trial to a nationally representative population of participants eligible for lung cancer screening.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/JCS39J6N/Steingrimsson - 2022 - Extending prediction models for use in a new targe.pdf}
}

@online{steingrimsson_sensitivity_2023,
  title = {Sensitivity Analysis for Studies Transporting Prediction Models},
  author = {Steingrimsson, Jon A. and Robertson, Sarah E. and Dahabreh, Issa J.},
  date = {2023-06-13},
  eprint = {2306.08084},
  eprinttype = {arxiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/2306.08084},
  urldate = {2023-07-26},
  abstract = {We consider the estimation of measures of model performance in a target population when covariate and outcome data are available on a sample from some source population and covariate data, but not outcome data, are available on a simple random sample from the target population. When outcome data are not available from the target population, identification of measures of model performance is possible under an untestable assumption that the outcome and population (source or target population) are independent conditional on covariates. In practice, this assumption is uncertain and, in some cases, controversial. Therefore, sensitivity analysis may be useful for examining the impact of assumption violations on inferences about model performance. Here, we propose an exponential tilt sensitivity analysis model and develop statistical methods to determine how sensitive measures of model performance are to violations of the assumption of conditional independence between outcome and population. We provide identification results and estimators for the risk in the target population, examine the large-sample properties of the estimators, and apply the estimators to data on individuals with stable ischemic heart disease.},
  pubstate = {preprint},
  keywords = {Statistics - Methodology},
  file = {/Users/christopherboyer/Zotero/storage/IUXZ9CRH/Steingrimsson et al. - 2023 - Sensitivity analysis for studies transporting pred.pdf;/Users/christopherboyer/Zotero/storage/UJQX5IX5/2306.html}
}

@online{steingrimsson_transporting_2021,
  title = {Transporting a Prediction Model for Use in a New Target Population},
  author = {Steingrimsson, Jon A. and Gatsonis, Constantine and Dahabreh, Issa J.},
  date = {2021-04-14},
  eprint = {2101.11182},
  eprinttype = {arxiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/2101.11182},
  urldate = {2022-12-15},
  abstract = {We consider methods for transporting a prediction model and assessing its performance for use in a new target population, when outcome and covariate information for model development is available from a simple random sample from the source population, but only covariate information is available on a simple random sample from the target population. We discuss how to tailor the prediction model for use in the target population, how to assess model performance in the target population (e.g., by estimating the target population mean squared error), and how to perform model and tuning parameter selection in the context of the target population. We provide identifiability results for the target population mean squared error of a potentially misspecified prediction model under a sampling design where the source study and the target population samples are obtained separately. We also introduce the concept of prediction error modifiers that can be used to reason about the need for tailoring measures of model performance to the target population and provide an illustration of the methods using simulated data.},
  pubstate = {preprint},
  keywords = {Statistics - Applications},
  file = {/Users/christopherboyer/Zotero/storage/2IAXZKVU/Steingrimsson et al. - 2021 - Transporting a prediction model for use in a new t.pdf;/Users/christopherboyer/Zotero/storage/LPJGGJPX/2101.html}
}

@article{steingrimsson_transporting_2023,
  title = {Transporting a {{Prediction Model}} for {{Use}} in a {{New Target Population}}},
  author = {Steingrimsson, Jon A and Gatsonis, Constantine and Li, Bing and Dahabreh, Issa J},
  date = {2023-02-01},
  journaltitle = {American Journal of Epidemiology},
  volume = {192},
  number = {2},
  pages = {296--304},
  issn = {0002-9262},
  doi = {10.1093/aje/kwac128},
  url = {https://doi.org/10.1093/aje/kwac128},
  urldate = {2023-03-15},
  abstract = {We considered methods for transporting a prediction model for use in a new target population, both when outcome and covariate data for model development are available from a source population that has a different covariate distribution compared with the target population and when covariate data (but not outcome data) are available from the target population. We discuss how to tailor the prediction model to account for differences in the data distribution between the source population and the target population. We also discuss how to assess the model’s performance (e.g., by estimating the mean squared prediction error) in the target population. We provide identifiability results for measures of model performance in the target population for a potentially misspecified prediction model under a sampling design where the source and the target population samples are obtained separately. We introduce the concept of prediction error modifiers that can be used to reason about tailoring measures of model performance to the target population. We illustrate the methods in simulated data and apply them to transport a prediction model for lung cancer diagnosis from the National Lung Screening Trial to the nationally representative target population of trial-eligible individuals in the National Health and Nutrition Examination Survey.},
  file = {/Users/christopherboyer/Zotero/storage/9L8VCNKE/Steingrimsson et al. - 2023 - Transporting a Prediction Model for Use in a New T.pdf;/Users/christopherboyer/Zotero/storage/VLC92CF9/6648776.html}
}

@article{steyerberg_assessing_2010,
  title = {Assessing the {{Performance}} of {{Prediction Models}}: {{A Framework}} for {{Traditional}} and {{Novel Measures}}},
  shorttitle = {Assessing the {{Performance}} of {{Prediction Models}}},
  author = {Steyerberg, Ewout W. and Vickers, Andrew J. and Cook, Nancy R. and Gerds, Thomas and Gonen, Mithat and Obuchowski, Nancy and Pencina, Michael J. and Kattan, Michael W.},
  date = {2010},
  journaltitle = {Epidemiology},
  volume = {21},
  number = {1},
  eprint = {25662818},
  eprinttype = {jstor},
  pages = {128--138},
  publisher = {{Lippincott Williams \& Wilkins}},
  issn = {1044-3983},
  url = {https://www.jstor.org/stable/25662818},
  urldate = {2020-08-17},
  abstract = {The performance of prediction models can be assessed using a variety of methods and metrics. Traditional measures for binary and survival outcomes include the Brier score to indicate overall model performance, the concordance (or c) statistic for discriminative ability (or area under the receiver operating characteristic [ROC] curve), and goodness-of-fit statistics for calibration. Several new measures have recently been proposed that can be seen as refinements of discrimination measures, including variants of the c statistic for survival, reclassification tables, net reclassification improvement (NRI), and integrated discrimination improvement (IDI). Moreover, decision—analytic measures have been proposed, including decision curves to plot the net benefit achieved by making decisions based on model predictions. We aimed to define the role of these relatively novel approaches in the evaluation of the performance of prediction models. For illustration, we present a case study of predicting the presence of residual tumor versus benign tissue in patients with testicular cancer (n = 544 for model development, n = 273 for external validation). We suggest that reporting discrimination and calibration will always be important for a prediction model. Decision-analytic measures should be reported if the predictive model is to be used for clinical decisions. Other measures of performance may be warranted in specific applications, such as reclassification metrics to gain insight into the value of adding a novel predictor to an established model.},
  file = {/Users/christopherboyer/Zotero/storage/K4K5HKJG/Steyerberg et al_2010_Assessing the Performance of Prediction Models.pdf}
}

@book{steyerberg_clinical_2019,
  title = {Clinical {{Prediction Models}}: {{A Practical Approach}} to {{Development}}, {{Validation}}, and {{Updating}}},
  shorttitle = {Clinical {{Prediction Models}}},
  author = {Steyerberg, Ewout W.},
  date = {2019},
  series = {Statistics for {{Biology}} and {{Health}}},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-16399-0},
  url = {http://link.springer.com/10.1007/978-3-030-16399-0},
  urldate = {2020-11-12},
  isbn = {978-3-030-16398-3 978-3-030-16399-0},
  langid = {english},
  file = {/Users/christopherboyer/Library/Mobile Documents/ZP9ZJ4EF3S~com~gingerlabs~Notability/Documents/2019_Book_ClinicalPredictionModels.nbn;/Users/christopherboyer/Zotero/storage/D9ZQ4QKU/Steyerberg - 2019 - Clinical Prediction Models A Practical Approach t.pdf}
}

@article{subbaswamy_development_2020,
  title = {From Development to Deployment: Dataset Shift, Causality, and Shift-Stable Models in Health {{AI}}},
  shorttitle = {From Development to Deployment},
  author = {Subbaswamy, Adarsh and Saria, Suchi},
  date = {2020-04-01},
  journaltitle = {Biostatistics},
  volume = {21},
  number = {2},
  pages = {345--352},
  issn = {1465-4644},
  doi = {10.1093/biostatistics/kxz041},
  url = {https://doi.org/10.1093/biostatistics/kxz041},
  urldate = {2022-12-15},
  abstract = {The deployment of machine learning (ML) and statistical models is beginning to transform the practice of healthcare, with models now able to help clinicians diagnose conditions like pneumonia and skin cancer, and to predict which hospital patients are at risk of adverse events such as septic shock. A major concern, however, is that model performance is heavily tied to details particular to the dataset the model was developed on—even slight deviations from the training conditions can result in wildly different performance. For example, when researchers trained a model to diagnose pneumonia from chest X-rays using data from one health system, but evaluated on data from an external health system, they found the model performed significantly worse than it did internally (Zech and others, 2018). The model failed to generalize (i.e., predict accurately) due to the shifts between the training conditions (health system one) and the deployment/testing conditions (health system two). These shifts are very common when moving a model from the training phase to deployment and can take a variety of forms, including changes in patient demographics, disease prevalence, measurement timing, equipment, treatment patterns, and more. Beyond contributing to poor performance, failing to account for shifts can also lead to dangerous decisions in practice: the system can fail to diagnose severely ill patients or recommend harmful treatments. This problem of shifting conditions which prevent generalization is referred to as dataset shift (Quiñonero-Candela and others, 2009), and in this article, we explain what it is, why it occurs, give an overview of the types of existing solutions, and discuss open challenges that remain.},
  file = {/Users/christopherboyer/Zotero/storage/QCMXUZ45/Subbaswamy and Saria - 2020 - From development to deployment dataset shift, cau.pdf;/Users/christopherboyer/Zotero/storage/GBBXXELY/5631850.html}
}

@article{sugiyama_covariate_2007,
  title = {Covariate {{Shift Adaptation}} by {{Importance Weighted Cross Validation}}},
  author = {Sugiyama, Masashi and Krauledat, Matthias and Müller, Klaus-Robert},
  date = {2007},
  journaltitle = {Journal of Machine Learning Research},
  volume = {8},
  number = {35},
  pages = {985--1005},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v8/sugiyama07a.html},
  urldate = {2022-12-15},
  abstract = {A common assumption in supervised learning is that the input points in the training set follow the same probability distribution as the input points that will be given in the future test phase. However, this assumption is not satisfied, for example, when the outside of the training region is extrapolated. The situation where the training input points and test input points follow different distributions while the conditional distribution of output values given input points is unchanged is called the covariate shift. Under the covariate shift, standard model selection techniques such as cross validation do not work as desired since its unbiasedness is no longer maintained. In this paper, we propose a new method called importance weighted cross validation (IWCV), for which we prove its unbiasedness even under the covariate shift. The IWCV procedure is the only one that can be applied for unbiased classification under covariate shift, whereas alternatives to IWCV exist for regression. The usefulness of our proposed method is illustrated by simulations, and furthermore demonstrated in the brain-computer interface, where strong non-stationarity effects can be seen between training and test sessions.},
  file = {/Users/christopherboyer/Zotero/storage/DUY9D24N/Sugiyama et al. - 2007 - Covariate Shift Adaptation by Importance Weighted .pdf}
}

@article{sugiyama_covariate_nodate,
  title = {Covariate {{Shift Adaptation}} by {{Importance Weighted Cross Validation}}},
  author = {Sugiyama, Masashi and Krauledat, Matthias},
  abstract = {A common assumption in supervised learning is that the input points in the training set follow the same probability distribution as the input points that will be given in the future test phase. However, this assumption is not satisfied, for example, when the outside of the training region is extrapolated. The situation where the training input points and test input points follow different distributions while the conditional distribution of output values given input points is unchanged is called the covariate shift. Under the covariate shift, standard model selection techniques such as cross validation do not work as desired since its unbiasedness is no longer maintained. In this paper, we propose a new method called importance weighted cross validation (IWCV), for which we prove its unbiasedness even under the covariate shift. The IWCV procedure is the only one that can be applied for unbiased classification under covariate shift, whereas alternatives to IWCV exist for regression. The usefulness of our proposed method is illustrated by simulations, and furthermore demonstrated in the brain-computer interface, where strong non-stationarity effects can be seen between training and test sessions.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/PEEAJH9M/Sugiyama and Krauledat - Covariate Shift Adaptation by Importance Weighted .pdf}
}

@book{sugiyama2012machine,
  title = {Machine Learning in Non-Stationary Environments: {{Introduction}} to Covariate Shift Adaptation},
  author = {Sugiyama, Masashi and Kawanabe, Motoaki},
  date = {2012},
  publisher = {{MIT press}}
}

@online{tchetgen_introduction_2020,
  title = {An {{Introduction}} to {{Proximal Causal Learning}}},
  author = {Tchetgen, Eric J. Tchetgen and Ying, Andrew and Cui, Yifan and Shi, Xu and Miao, Wang},
  date = {2020-09-23},
  eprint = {2009.10982},
  eprinttype = {arxiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/2009.10982},
  urldate = {2022-12-15},
  abstract = {A standard assumption for causal inference from observational data is that one has measured a sufficiently rich set of covariates to ensure that within covariate strata, subjects are exchangeable across observed treatment values. Skepticism about the exchangeability assumption in observational studies is often warranted because it hinges on investigators' ability to accurately measure covariates capturing all potential sources of confounding. Realistically, confounding mechanisms can rarely if ever, be learned with certainty from measured covariates. One can therefore only ever hope that covariate measurements are at best proxies of true underlying confounding mechanisms operating in an observational study, thus invalidating causal claims made on basis of standard exchangeability conditions. Causal learning from proxies is a challenging inverse problem which has to date remained unresolved. In this paper, we introduce a formal potential outcome framework for proximal causal learning, which while explicitly acknowledging covariate measurements as imperfect proxies of confounding mechanisms, offers an opportunity to learn about causal effects in settings where exchangeability on the basis of measured covariates fails. Sufficient conditions for nonparametric identification are given, leading to the proximal g-formula and corresponding proximal g-computation algorithm for estimation. These may be viewed as generalizations of Robins' foundational g-formula and g-computation algorithm, which account explicitly for bias due to unmeasured confounding. Both point treatment and time-varying treatment settings are considered, and an application of proximal g-computation of causal effects is given for illustration.},
  pubstate = {preprint},
  keywords = {62A01,Statistics - Methodology},
  file = {/Users/christopherboyer/Zotero/storage/5E2UM395/Tchetgen et al. - 2020 - An Introduction to Proximal Causal Learning.pdf;/Users/christopherboyer/Zotero/storage/DNQT46XW/2009.html}
}

@article{van_geloven_prediction_2020,
  title = {Prediction Meets Causal Inference: The Role of Treatment in Clinical Prediction Models},
  shorttitle = {Prediction Meets Causal Inference},
  author = {van Geloven, Nan and Swanson, Sonja A. and Ramspek, Chava L. and Luijken, Kim and van Diepen, Merel and Morris, Tim P. and Groenwold, Rolf H. H. and van Houwelingen, Hans C. and Putter, Hein and le Cessie, Saskia},
  options = {useprefix=true},
  date = {2020-07-01},
  journaltitle = {Eur J Epidemiol},
  volume = {35},
  number = {7},
  pages = {619--630},
  issn = {1573-7284},
  doi = {10.1007/s10654-020-00636-1},
  url = {https://doi.org/10.1007/s10654-020-00636-1},
  urldate = {2020-08-12},
  abstract = {In this paper we study approaches for dealing with treatment when developing a clinical prediction model. Analogous to the estimand framework recently proposed by the European Medicines Agency for clinical trials, we propose a ‘predictimand’ framework of different questions that may be of interest when predicting risk in relation to treatment started after baseline. We provide a formal definition of the estimands matching these questions, give examples of settings in which each is useful and discuss appropriate estimators including their assumptions. We illustrate the impact of the predictimand choice in a dataset of patients with end-stage kidney disease. We argue that clearly defining the estimand is equally important in prediction research as in causal inference.},
  langid = {english},
  file = {/Users/christopherboyer/Zotero/storage/MRJTJ2XN/van Geloven et al_2020_Prediction meets causal inference.pdf}
}

@book{van2003unified,
  title = {Unified Methods for Censored Longitudinal Data and Causality},
  author = {Van der Laan, Mark J and Robins, James M},
  date = {2003},
  volume = {5},
  publisher = {{Springer}}
}

@online{xu_calibration_2022,
  title = {Calibration {{Error}} for {{Heterogeneous Treatment Effects}}},
  author = {Xu, Yizhe and Yadlowsky, Steve},
  date = {2022-03-24},
  eprint = {2203.13364},
  eprinttype = {arxiv},
  eprintclass = {stat},
  url = {http://arxiv.org/abs/2203.13364},
  urldate = {2022-12-15},
  abstract = {Recently, many researchers have advanced data-driven methods for modeling heterogeneous treatment effects (HTEs). Even still, estimation of HTEs is a difficult task -- these methods frequently over- or under-estimate the treatment effects, leading to poor calibration of the resulting models. However, while many methods exist for evaluating the calibration of prediction and classification models, formal approaches to assess the calibration of HTE models are limited to the calibration slope. In this paper, we define an analogue of the \textbackslash smash\{(\$\textbackslash ell\_2\$)\} expected calibration error for HTEs, and propose a robust estimator. Our approach is motivated by doubly robust treatment effect estimators, making it unbiased, and resilient to confounding, overfitting, and high-dimensionality issues. Furthermore, our method is straightforward to adapt to many structures under which treatment effects can be identified, including randomized trials, observational studies, and survival analysis. We illustrate how to use our proposed metric to evaluate the calibration of learned HTE models through the application to the CRITEO-UPLIFT Trial.},
  pubstate = {preprint},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/christopherboyer/Zotero/storage/TBQ3NVJK/Xu and Yadlowsky - 2022 - Calibration Error for Heterogeneous Treatment Effe.pdf;/Users/christopherboyer/Zotero/storage/TSRIVP8P/2203.html}
}
